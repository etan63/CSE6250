{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64cda73",
   "metadata": {},
   "source": [
    "This notebook contains the code to run the variationally regularized GNN with additional modifications to help with VRAM consumption. Originally the authors had a P100 (16gb VRAM) to run their models, and the code itself was unoptimized for GPUs with smaller VRAMs. Hence taking advantage of the newer versions of PyTorch as well as the developments in 3rd party libraries, a more VRAM efficient version of the code has been produced. \n",
    "\n",
    "In addition, improvements such as residual connections to the attention mechanism was also implemented as it is known to help with the performance (something that the authors were missing)\n",
    "\n",
    "Furthermore dynamic GPU VRAM management was also implemented to prevent the VRAM usage from exploding and leading to either OOM errors or shared GPU VRAM usage (shifting data between GPU and RAM lead to ~2-3x slowdown).\n",
    "\n",
    "More details can be found on the Github page, where the scientific improvements (eg. Focal loss, Residual connections) and as well as the engineering (eg. dynamic vram management, downcasting to fp16) are explained. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8b924c",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254f7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Import all torch + DL related libraries\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import bitsandbytes as bnb\n",
    "# Import other libraries\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "# For aggregate logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a68a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device to be cuda if cuda GPU exists\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842c39b",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e5a1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engmeng\\AppData\\Local\\Temp\\ipykernel_3160\\187951048.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Use AMP to reduce VRAM load and to speed up training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Modify the train function to use autocast and return the loss as a Tensor for scaling\n",
    "def train(data, model, optim, criterion, lbd, max_clip_norm=5):\n",
    "    model.train()\n",
    "    input = data[:, :-1].to(device)\n",
    "    label = data[:, -1].float().to(device)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    \n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):  # Enable AMP here\n",
    "        logits, kld = model(input)\n",
    "        logits = logits.squeeze(-1)\n",
    "        kld = kld.sum()\n",
    "        \n",
    "        bce = criterion(logits, label)\n",
    "        loss = bce + lbd * kld\n",
    "    \n",
    "    # Scale the loss before backward pass\n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    # Apply gradient clipping only when needed\n",
    "    if max_clip_norm:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_clip_norm)\n",
    "    \n",
    "    # Update the optimizer with scaled gradients\n",
    "    scaler.step(optim)\n",
    "    scaler.update()  # Adjust scaler for next iteration\n",
    "    \n",
    "    return loss.item(), kld.item(), bce.item()\n",
    "\n",
    "\n",
    "def evaluate(model, data_iter, length):\n",
    "    model.eval()\n",
    "    y_pred = np.zeros(length)\n",
    "    y_true = np.zeros(length)\n",
    "    y_prob = np.zeros(length)\n",
    "    pointer = 0\n",
    "    for data in data_iter:\n",
    "        input = data[:, :-1].to(device)\n",
    "        label = data[:, -1]\n",
    "        batch_size = len(label)\n",
    "        probability, _ = model(input)\n",
    "        probability = torch.sigmoid(probability.squeeze(-1).detach())\n",
    "        predicted = probability > 0.5\n",
    "        y_true[pointer: pointer + batch_size] = label.cpu().numpy()\n",
    "        y_pred[pointer: pointer + batch_size] = predicted.cpu().numpy()\n",
    "        y_prob[pointer: pointer + batch_size] = probability.cpu().numpy()\n",
    "        pointer += batch_size\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    return auc(recall, precision), (y_pred, y_prob, y_true), (recall, precision)\n",
    "\n",
    "class EHRData(Dataset):\n",
    "    def __init__(self, data, cla):\n",
    "        self.data = data\n",
    "        self.cla = cla\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cla)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.cla[idx]\n",
    "\n",
    "# Reduce the for loops as much as possible\n",
    "def collate_fn(data):\n",
    "    # Convert the sparse matrices to dense arrays in a batch operation\n",
    "    features = np.array([datum[0].toarray().ravel() for datum in data], dtype=np.float32)\n",
    "    labels = np.array([datum[1] for datum in data], dtype=np.float32).reshape(-1, 1)\n",
    "    \n",
    "    # Stack features and labels along the last axis\n",
    "    data_combined = np.hstack((features, labels))\n",
    "\n",
    "    # Convert directly to a PyTorch tensor\n",
    "    return torch.from_numpy(data_combined).long().to(device)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='sum'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Compute the binary cross entropy with logits\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        # Compute the Focal Loss\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        # Allow for different types of calculation for focal loss\n",
    "        if self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def check_and_clear_vram(threshold=0.9):\n",
    "    # Get total and reserved memory\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory\n",
    "    used_vram = torch.cuda.memory_reserved(0)\n",
    "    usage_ratio = used_vram / total_vram\n",
    "    \n",
    "    # Clear cache if VRAM usage exceeds threshold\n",
    "    if usage_ratio >= threshold:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return usage_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a47e677",
   "metadata": {},
   "source": [
    "## Graph Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3851652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def clone_params(param, N):\n",
    "    return nn.ParameterList([copy.deepcopy(param) for _ in range(N)])\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class GraphLayer(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_of_nodes,\n",
    "                 num_of_heads, dropout, alpha, concat=True):\n",
    "        super(GraphLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        self.num_of_nodes = num_of_nodes\n",
    "        self.num_of_heads = num_of_heads\n",
    "\n",
    "        # Single W Linear layer for all heads\n",
    "        self.W = nn.Linear(in_features, hidden_features * num_of_heads, bias=False)\n",
    "        self.a = nn.Parameter(torch.rand((num_of_heads, 2 * hidden_features), requires_grad=True))\n",
    "\n",
    "        # Define V based on whether heads are concatenated\n",
    "        self.V = nn.Linear(num_of_heads * hidden_features if concat else hidden_features, out_features)\n",
    "\n",
    "        # Dropout and LeakyReLU\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm = LayerNorm(num_of_heads * hidden_features if concat else hidden_features)\n",
    "\n",
    "    def initialize(self):\n",
    "        nn.init.xavier_normal_(self.W.weight.data)\n",
    "        nn.init.xavier_normal_(self.a.data)\n",
    "        nn.init.xavier_normal_(self.V.weight.data)\n",
    "\n",
    "    def attention(self, N, data, edge):\n",
    "        # Project data to (N, num_heads, hidden_features)\n",
    "        data_proj = self.W(data).view(N, self.num_of_heads, self.hidden_features)\n",
    "\n",
    "        # Gather source and destination features for each edge\n",
    "        edge_src, edge_dst = edge\n",
    "        h_src = data_proj[edge_src, :, :]  # (E, num_heads, hidden_features)\n",
    "        h_dst = data_proj[edge_dst, :, :]  # (E, num_heads, hidden_features)\n",
    "\n",
    "        # Concatenate features of edge endpoints and compute attention scores\n",
    "        # Standard attention structure using concat\n",
    "        edge_h = torch.cat([h_src, h_dst], dim=-1)  # (E, num_heads, 2 * hidden_features)\n",
    "        edge_e = self.leakyrelu((self.a.unsqueeze(0) * edge_h).sum(dim=-1))  # (E, num_heads)\n",
    "\n",
    "        e_rowsum = torch.zeros((N, self.num_of_heads), device=data.device)  # Shape: (N, num_heads)\n",
    "        h_prime = torch.zeros((N, self.num_of_heads, self.hidden_features), device=data.device)  # Shape: (N, num_heads, hidden_features)\n",
    "\n",
    "        # Aggregate across all edges in one pass to improve efficiency\n",
    "        e_rowsum.index_add_(0, edge_dst, edge_e)  # Shape: (N, num_heads)\n",
    "        h_prime.index_add_(0, edge_dst, edge_e.unsqueeze(-1) * h_src)  # Shape: (N, num_heads, hidden_features)\n",
    "\n",
    "        # Normalize in-place to avoid creating new tensors\n",
    "        e_rowsum.clamp_(min=1.0)  # Prevent division by zero\n",
    "        h_prime.div_(e_rowsum.unsqueeze(-1))\n",
    "\n",
    "        return h_prime\n",
    "\n",
    "    def forward(self, edge, data):\n",
    "        N = self.num_of_nodes\n",
    "        h_prime = self.attention(N, data, edge)\n",
    "\n",
    "        # Concatenate or average heads based on `concat`\n",
    "        if self.concat:\n",
    "            h_prime = h_prime.view(N, -1)  # Concatenate heads (N, num_heads * hidden_features)\n",
    "            h_prime = F.elu(self.norm(h_prime))  # Apply ELU activation in-place\n",
    "        else:\n",
    "            h_prime = self.V(F.relu(self.norm(h_prime.mean(dim=1))))  # Apply ReLU activation in-place\n",
    "\n",
    "        # Apply dropout\n",
    "        h_prime = self.dropout(h_prime)\n",
    "\n",
    "        return h_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7510ae6",
   "metadata": {},
   "source": [
    "## Variational GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8639f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gradient checkpointing - conditional -90% vram to speed up - residual connections -ok and good\n",
    "#This isnt really giving the speed up unfortunately\n",
    "def conditional_checkpoint(layer, *inputs):\n",
    "    # Calculate current VRAM usage\n",
    "    vram_usage_ratio = torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory\n",
    "    # Apply checkpointing if VRAM usage exceeds the threshold\n",
    "    if vram_usage_ratio > VRAM_THRESHOLD:\n",
    "        return checkpoint(layer, *inputs)\n",
    "    else:\n",
    "        return layer(*inputs)\n",
    "\n",
    "class VariationalGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, num_of_nodes, n_heads, n_layers,\n",
    "                 dropout, alpha, variational=True, none_graph_features=0, concat=True):\n",
    "        super(VariationalGNN, self).__init__()\n",
    "        self.variational = variational\n",
    "        self.num_of_nodes = num_of_nodes + 1 - none_graph_features\n",
    "        self.embed = nn.Embedding(self.num_of_nodes, in_features, padding_idx=0)\n",
    "        self.in_att = clones(\n",
    "            GraphLayer(in_features, in_features, in_features, self.num_of_nodes,\n",
    "                       n_heads, dropout, alpha, concat=True), n_layers)\n",
    "        self.out_features = out_features\n",
    "        self.out_att = GraphLayer(in_features, in_features, out_features, self.num_of_nodes,\n",
    "                                  n_heads, dropout, alpha, concat=False)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.parameterize = nn.Linear(out_features, out_features * 2)\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, 1))\n",
    "        self.none_graph_features = none_graph_features\n",
    "        if none_graph_features > 0:\n",
    "            self.features_ffn = nn.Sequential(\n",
    "                nn.Linear(none_graph_features, out_features // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "            self.out_layer = nn.Sequential(\n",
    "                nn.Linear(out_features + out_features // 2, out_features),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(out_features, 1))\n",
    "        for i in range(n_layers):\n",
    "            self.in_att[i].initialize()\n",
    "\n",
    "    def data_to_edges(self, data):\n",
    "        # Convert data to edges with device allocation at the end\n",
    "        data = data.bool()\n",
    "        length = data.size()[0]\n",
    "        nonzero = data.nonzero(as_tuple=False)\n",
    "        \n",
    "        if nonzero.numel() == 0:\n",
    "            return torch.LongTensor([[0], [0]]).to(device), torch.LongTensor([[length + 1], [length + 1]]).to(device)\n",
    "        \n",
    "        if self.training:\n",
    "            mask = (torch.rand(nonzero.size(0), device=data.device) > 0.05)\n",
    "            nonzero = nonzero[mask]\n",
    "            if nonzero.numel() == 0:\n",
    "                return torch.LongTensor([[0], [0]]).to(device), torch.LongTensor([[length + 1], [length + 1]]).to(device)\n",
    "        \n",
    "        nonzero = nonzero.T + 1\n",
    "        lengths = nonzero.size(1)\n",
    "        \n",
    "        input_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                 nonzero.repeat(lengths, 1).T.contiguous().view(1, lengths ** 2)), dim=0)\n",
    "        \n",
    "        # Extend nonzero and avoid redundant device transfer\n",
    "        nonzero = torch.cat((nonzero, torch.LongTensor([[length + 1]]).to(data.device)), dim=1)\n",
    "        lengths = nonzero.size(1)\n",
    "        output_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                  nonzero.repeat(lengths, 1).T.contiguous().view(1, lengths ** 2)), dim=0)\n",
    "        return input_edges, output_edges\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = (0.5 * logvar).exp()\n",
    "            eps = torch.randn_like(std, device=mu.device)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        return mu\n",
    "\n",
    "    def encoder_decoder(self, data):\n",
    "        # Calculate edges\n",
    "        input_edges, output_edges = self.data_to_edges(data)\n",
    "\n",
    "        # Embed the nodes\n",
    "        h_prime = self.embed(torch.arange(self.num_of_nodes, device=data.device).long())\n",
    "        if h_prime.requires_grad is False:\n",
    "            h_prime.requires_grad = True\n",
    "\n",
    "        # Apply gradient checkpointing with VRAM monitoring\n",
    "        for attn in self.in_att:\n",
    "            if torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory > 0.9:\n",
    "                torch.cuda.empty_cache()  # Conditionally clear cache when VRAM is > 90%\n",
    "\n",
    "            # Residual connection for each layer in in_att\n",
    "            h_prime_res = h_prime\n",
    "            h_prime = conditional_checkpoint(attn, input_edges, h_prime) + h_prime_res  # Add residual connection but with conditional checkpointing\n",
    "\n",
    "        # Variational encoding step\n",
    "        if self.variational:\n",
    "            h_prime = self.parameterize(h_prime).view(-1, 2, self.out_features)\n",
    "            mu, logvar = h_prime[:, 0, :], h_prime[:, 1, :]\n",
    "            h_prime = self.reparameterise(mu, logvar)\n",
    "            mu, logvar = mu[data], logvar[data]\n",
    "\n",
    "        # Residual connection for the out_att layer as well\n",
    "        h_prime_res = h_prime\n",
    "        if torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory > 0.9:\n",
    "            torch.cuda.empty_cache()  # Clear cache if VRAM > 90%\n",
    "        h_prime = checkpoint(self.out_att, output_edges, h_prime) + h_prime_res  # Add residual connection\n",
    "\n",
    "        kld = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)) / mu.size(0) if self.variational else 0\n",
    "        return h_prime[-1], kld\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size = data.size(0)\n",
    "        if self.none_graph_features == 0:\n",
    "            outputs = [self.encoder_decoder(data[i]) for i in range(batch_size)]\n",
    "            outputs_h, kld_sum = zip(*outputs)\n",
    "            return self.out_layer(F.relu(torch.stack(outputs_h))), torch.sum(torch.tensor(kld_sum))\n",
    "        else:\n",
    "            outputs = [(data[i, :self.none_graph_features],\n",
    "                        self.encoder_decoder(data[i, self.none_graph_features:])) for i in range(batch_size)]\n",
    "            outputs_h, kld_sum = zip(*[(self.features_ffn(torch.FloatTensor([out[0]]).to(device)), out[1][0]) for out in outputs])\n",
    "            return self.out_layer(F.relu(torch.stack(outputs_h))), torch.sum(torch.tensor(kld_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353585c6",
   "metadata": {},
   "source": [
    "## Training + Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e2f1f",
   "metadata": {},
   "source": [
    "## Standard Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "VRAM_THRESHOLD = 0.9  # 90% usage (prevent leak or OOM dynamically to improve training throughput)\n",
    "# Folder paths\n",
    "result_path = \"./output/focal/\"\n",
    "data_path = \"./Preprocessing/mimic/output/\"\n",
    "# Hyperparameters\n",
    "embedding_size = 32\n",
    "in_feature = embedding_size\n",
    "out_feature =embedding_size\n",
    "n_layers = 2\n",
    "lr = 1e-4\n",
    "reg = True\n",
    "n_heads = 1\n",
    "dropout = 0.2\n",
    "alpha = 0.1\n",
    "batch_size = 256\n",
    "number_of_epochs = 2\n",
    "lbd = 1\n",
    "focal_loss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5fe39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(result_path,data_path,in_feature,out_feature,n_layers,reg,n_heads,\n",
    "                  alpha,batch_size,number_of_epochs,lbd,VRAM_THRESHOLD,lr,dropout,focal_loss_val\n",
    "                  ):\n",
    "    # Load data\n",
    "    train_x, train_y = pickle.load(open(data_path + '/train_csr.pkl', 'rb'))\n",
    "    val_x, val_y = pickle.load(open(data_path + '/validation_csr.pkl', 'rb'))\n",
    "    test_x, test_y = pickle.load(open(data_path + '/test_csr.pkl', 'rb'))\n",
    "\n",
    "    # Upsample training data\n",
    "    train_upsampling = np.concatenate((np.arange(len(train_y)), np.repeat(np.where(train_y == 1)[0], 1)))\n",
    "    train_x = train_x[train_upsampling]\n",
    "    train_y = train_y[train_upsampling]\n",
    "\n",
    "    # Create result root\n",
    "    s = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    if focal_loss_val == True:\n",
    "        result_root = f'{result_path}/lr_{lr}-input_{embedding_size}-output_{embedding_size}-dropout_{dropout}-focal_True'\n",
    "    else:\n",
    "        result_root = f'{result_path}/lr_{lr}-input_{embedding_size}-output_{embedding_size}-dropout_{dropout}-focal_False'\n",
    "    os.makedirs(result_root, exist_ok=True)\n",
    "    logging.basicConfig(filename=f'{result_root}/train.log', format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "    logging.info(f\"Time: {s}\")\n",
    "\n",
    "    # Initialize model\n",
    "    num_of_nodes = train_x.shape[1] + 1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VariationalGNN(embedding_size, embedding_size, num_of_nodes, n_heads, n_layers - 1,\n",
    "                           dropout=dropout, alpha=alpha, variational=reg, none_graph_features=0).to(device)\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    val_loader = DataLoader(dataset=EHRData(val_x, val_y), batch_size=batch_size,\n",
    "                            collate_fn=collate_fn, shuffle=False\n",
    "                           )\n",
    "    # 8 bit optimizer to speed things up and reduce memory load\n",
    "    optimizer = bnb.optim.Adam8bit(\n",
    "        [p for p in model.parameters() if p.requires_grad], lr=lr, weight_decay=1e-8\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    pr_list = []\n",
    "    val_list = []\n",
    "    for epoch in range(number_of_epochs):\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "        ratio = Counter(train_y)\n",
    "        train_loader = DataLoader(dataset=EHRData(train_x, train_y), batch_size=batch_size,\n",
    "                                  collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "        if focal_loss_val == True:\n",
    "            # Initialize the Focal Loss criterion with calculated pos_weight\n",
    "            pos_weight = ratio[False] / ratio[True]\n",
    "            criterion = FocalLoss(alpha=pos_weight, gamma=1.5, reduction=\"sum\").to(device)  # Modify gamma and alpha as needed\n",
    "        else:\n",
    "            pos_weight = torch.ones(1).float().to(device) * (ratio[False] / ratio[True])\n",
    "            criterion = nn.BCEWithLogitsLoss(reduction=\"sum\", pos_weight=pos_weight)\n",
    "\n",
    "        t = tqdm(iter(train_loader), leave=False, total=len(train_loader))\n",
    "        model.train()\n",
    "        total_loss = np.zeros(3)\n",
    "\n",
    "        for idx, batch_data in enumerate(t):\n",
    "            # Train the model on this batch\n",
    "            loss, kld, focal_loss = train(batch_data, model, optimizer, criterion, lbd, max_clip_norm=5)\n",
    "            total_loss += np.array([loss, focal_loss, kld])\n",
    "\n",
    "            # Check VRAM usage and clear cache if needed\n",
    "            vram_usage = check_and_clear_vram(VRAM_THRESHOLD)  # Check VRAM only when threshold is met\n",
    "\n",
    "            # Periodic evaluation and saving\n",
    "            if idx % (len(train_loader)-1) == 0 and idx > 0:\n",
    "                torch.save(model.state_dict(), f\"{result_root}/parameter{epoch}_{idx}\")\n",
    "\n",
    "                # Free resources before evaluation\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    val_auprc, _,prc = evaluate(model, val_loader, len(val_y))\n",
    "                    pr_list.append(prc)\n",
    "                    val_list.append(val_auprc)\n",
    "\n",
    "                logging.info(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, focal_loss: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "                print(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, focal_loss: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "\n",
    "                del val_auprc  # Clear evaluation results\n",
    "                model.train()\n",
    "\n",
    "            # Update progress display\n",
    "            if idx % 50 == 0 and idx > 0:\n",
    "                t.set_description(f'[epoch:{epoch + 1}] loss: {total_loss[0]/idx:.4f}, focal_loss: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "                t.refresh()\n",
    "\n",
    "        # Update scheduler and check VRAM at the end of each epoch\n",
    "        scheduler.step()\n",
    "        check_and_clear_vram()\n",
    "\n",
    "    data = []\n",
    "    for epoch_idx, (recall,precision) in enumerate(pr_list, start=1):\n",
    "        data.append({\n",
    "            \"Embedding_Size\": embedding_size,\n",
    "            \"Learning_Rate\" : lr,\n",
    "            \"Dropout\": dropout,\n",
    "            \"Alpha\":alpha,\n",
    "            \"Focal\" : str(focal_loss_val),\n",
    "            \"epoch\": epoch_idx,\n",
    "            \"precision\": precision.tolist(),\n",
    "            \"recall\": recall.tolist(),\n",
    "            \"auprc\": val_list[epoch_idx - 1]\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(f'{result_root}/summary_runs.xlsx', index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905cf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment here to generate a single run based on the parameters\n",
    "# df_run = train_evaluate(result_path,data_path,in_feature,out_feature,n_layers,reg,n_heads,\n",
    "#                   alpha,batch_size,number_of_epochs,lbd,VRAM_THRESHOLD,lr,dropout,focal_loss\n",
    "#                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b8e1fa",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b606a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 40.6395, focal_loss: 17.4474, kld: 23.1921: 100%|█████████████████▉| 1389/1390 [08:00<00:00,  2.87it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.5644101276217018; loss: 40.5494, focal_loss: 17.3645, kld: 23.1849\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 37.3654, focal_loss: 13.5793, kld: 23.7860: 100%|█████████████████▉| 1389/1390 [07:57<00:00,  2.74it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.6478064945774141; loss: 37.3423, focal_loss: 13.5659, kld: 23.7764\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 36.5528, focal_loss: 12.6354, kld: 23.9173: 100%|█████████████████▉| 1389/1390 [07:59<00:00,  2.89it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.6700081306428388; loss: 36.5557, focal_loss: 12.6496, kld: 23.9061\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 36.0401, focal_loss: 11.8096, kld: 24.2305: 100%|█████████████████▉| 1389/1390 [08:01<00:00,  2.87it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.6876039723505643; loss: 35.9975, focal_loss: 11.7791, kld: 24.2184\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 35.2930, focal_loss: 10.9277, kld: 24.3653: 100%|█████████████████▉| 1389/1390 [08:04<00:00,  2.90it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.6937254915600751; loss: 35.2588, focal_loss: 10.9078, kld: 24.3510\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 34.7091, focal_loss: 10.1865, kld: 24.5225: 100%|█████████████████▉| 1389/1390 [08:02<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.6987698146256565; loss: 34.6928, focal_loss: 10.1841, kld: 24.5087\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 34.4973, focal_loss: 9.9681, kld: 24.5292: 100%|██████████████████▉| 1389/1390 [07:58<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.6959119629875391; loss: 34.4724, focal_loss: 9.9529, kld: 24.5195\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 34.2133, focal_loss: 9.4707, kld: 24.7426: 100%|██████████████████▉| 1389/1390 [07:58<00:00,  2.94it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.6965758504510567; loss: 34.1985, focal_loss: 9.4810, kld: 24.7175\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 34.0075, focal_loss: 9.1121, kld: 24.8954: 100%|██████████████████▉| 1389/1390 [07:58<00:00,  2.94it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.7019886950195212; loss: 33.9783, focal_loss: 9.0998, kld: 24.8785\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 33.9063, focal_loss: 8.7610, kld: 25.1453: 100%|█████████████████▉| 1389/1390 [07:59<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.7024944549411217; loss: 33.8938, focal_loss: 8.7641, kld: 25.1298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 50.2029, focal_loss: 24.4745, kld: 25.7284: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.56237119317267; loss: 50.0972, focal_loss: 24.3773, kld: 25.7199\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 45.2215, focal_loss: 19.1758, kld: 26.0457: 100%|█████████████████▉| 1389/1390 [07:57<00:00,  2.86it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.6373850814049984; loss: 45.1951, focal_loss: 19.1630, kld: 26.0321\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 42.8848, focal_loss: 16.7088, kld: 26.1760: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.6661345783489524; loss: 42.9156, focal_loss: 16.7427, kld: 26.1729\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 42.0217, focal_loss: 15.7009, kld: 26.3208: 100%|█████████████████▉| 1389/1390 [07:59<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.674259380385307; loss: 41.9879, focal_loss: 15.6945, kld: 26.2934\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 41.3885, focal_loss: 14.9249, kld: 26.4636: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.90it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.6830048704955447; loss: 41.3297, focal_loss: 14.8851, kld: 26.4446\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 40.6229, focal_loss: 13.8371, kld: 26.7858: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.68313875053233; loss: 40.5755, focal_loss: 13.8118, kld: 26.7637\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 40.2704, focal_loss: 13.2497, kld: 27.0207: 100%|█████████████████▉| 1389/1390 [07:59<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.6920049837353717; loss: 40.2536, focal_loss: 13.2495, kld: 27.0040\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 39.6064, focal_loss: 12.3650, kld: 27.2414: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.87it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.6979329147163902; loss: 39.5593, focal_loss: 12.3276, kld: 27.2317\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 39.4517, focal_loss: 11.9012, kld: 27.5505: 100%|█████████████████▉| 1389/1390 [07:59<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.6953166475524939; loss: 39.4337, focal_loss: 11.9055, kld: 27.5283\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 39.7587, focal_loss: 11.8886, kld: 27.8701: 100%|████████████████▉| 1389/1390 [07:58<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.6952002521813855; loss: 39.6903, focal_loss: 11.8324, kld: 27.8579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 38.9344, focal_loss: 17.7502, kld: 21.1842: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.5667807479573248; loss: 38.8134, focal_loss: 17.6467, kld: 21.1667\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 35.0291, focal_loss: 13.7675, kld: 21.2616: 100%|█████████████████▉| 1389/1390 [07:59<00:00,  2.92it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.6303976382129595; loss: 35.0011, focal_loss: 13.7504, kld: 21.2507\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 34.4700, focal_loss: 12.9538, kld: 21.5162: 100%|█████████████████▉| 1389/1390 [07:59<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.6607453670303814; loss: 34.4046, focal_loss: 12.8998, kld: 21.5048\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 33.3512, focal_loss: 11.5652, kld: 21.7860: 100%|█████████████████▉| 1389/1390 [07:59<00:00,  2.64it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.6824466824603983; loss: 33.3046, focal_loss: 11.5351, kld: 21.7696\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 33.0520, focal_loss: 11.0737, kld: 21.9782: 100%|█████████████████▉| 1389/1390 [07:59<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.6932675455442472; loss: 33.0600, focal_loss: 11.0845, kld: 21.9755\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 32.2887, focal_loss: 9.9514, kld: 22.3373: 100%|██████████████████▉| 1389/1390 [07:58<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.696388615259498; loss: 32.3097, focal_loss: 9.9884, kld: 22.3213\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 32.0669, focal_loss: 9.5736, kld: 22.4933: 100%|██████████████████▉| 1389/1390 [07:59<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.6976342035976585; loss: 32.0681, focal_loss: 9.5867, kld: 22.4813\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 31.9899, focal_loss: 9.2003, kld: 22.7895: 100%|██████████████████▉| 1389/1390 [07:58<00:00,  2.92it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.7109741638704775; loss: 31.9773, focal_loss: 9.1984, kld: 22.7789\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 31.7821, focal_loss: 8.8218, kld: 22.9603: 100%|██████████████████▉| 1389/1390 [07:59<00:00,  2.92it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.7068047652992151; loss: 31.7718, focal_loss: 8.8231, kld: 22.9487\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 31.9254, focal_loss: 8.6641, kld: 23.2613: 100%|█████████████████▉| 1389/1390 [07:59<00:00,  2.87it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.7062867928919042; loss: 31.9042, focal_loss: 8.6603, kld: 23.2439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 49.8211, focal_loss: 23.9156, kld: 25.9055: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.5869975966720999; loss: 49.7097, focal_loss: 23.8158, kld: 25.8939\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 44.8199, focal_loss: 18.8485, kld: 25.9715: 100%|█████████████████▉| 1389/1390 [07:57<00:00,  2.90it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.6513735385576712; loss: 44.8116, focal_loss: 18.8499, kld: 25.9617\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 43.4497, focal_loss: 17.1486, kld: 26.3011: 100%|█████████████████▉| 1389/1390 [07:57<00:00,  2.94it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.671733649194377; loss: 43.3853, focal_loss: 17.0942, kld: 26.2911\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 42.5202, focal_loss: 15.5778, kld: 26.9424: 100%|█████████████████▉| 1389/1390 [07:59<00:00,  2.87it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.6710504242875336; loss: 42.5590, focal_loss: 15.6209, kld: 26.9381\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 42.5994, focal_loss: 14.8644, kld: 27.7350: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.88it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.6774916612279194; loss: 42.6067, focal_loss: 14.8803, kld: 27.7264\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 41.6140, focal_loss: 13.3789, kld: 28.2350: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.6891055674247178; loss: 41.6517, focal_loss: 13.4328, kld: 28.2189\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 41.5755, focal_loss: 13.0960, kld: 28.4795: 100%|█████████████████▉| 1389/1390 [07:57<00:00,  2.94it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.6918224898240496; loss: 41.5237, focal_loss: 13.0639, kld: 28.4598\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 40.8334, focal_loss: 12.1132, kld: 28.7202: 100%|█████████████████▉| 1389/1390 [07:57<00:00,  2.88it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.6955084728191996; loss: 40.8082, focal_loss: 12.1059, kld: 28.7023\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 41.2278, focal_loss: 12.0524, kld: 29.1754: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.92it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.6981187421475402; loss: 41.1777, focal_loss: 12.0083, kld: 29.1694\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 41.0599, focal_loss: 11.2826, kld: 29.7773: 100%|████████████████▉| 1389/1390 [07:57<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.7006975103573143; loss: 41.0759, focal_loss: 11.3097, kld: 29.7661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 41.9047, focal_loss: 18.4415, kld: 23.4632: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.87it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.5332791104352094; loss: 41.7626, focal_loss: 18.3215, kld: 23.4411\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 37.2798, focal_loss: 13.9467, kld: 23.3331: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.89it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.634684797140691; loss: 37.2308, focal_loss: 13.9221, kld: 23.3087\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 35.9356, focal_loss: 12.7358, kld: 23.1998: 100%|█████████████████▉| 1389/1390 [08:00<00:00,  2.92it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.6616044492999374; loss: 35.8940, focal_loss: 12.7076, kld: 23.1864\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 35.3579, focal_loss: 12.0170, kld: 23.3409: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.92it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.6702036732925035; loss: 35.3262, focal_loss: 12.0032, kld: 23.3230\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 34.7918, focal_loss: 11.2605, kld: 23.5313: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.94it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.7002617424291004; loss: 34.7698, focal_loss: 11.2542, kld: 23.5156\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 34.1726, focal_loss: 10.5022, kld: 23.6703: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.7077552217933889; loss: 34.1353, focal_loss: 10.4798, kld: 23.6556\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 33.8570, focal_loss: 10.0709, kld: 23.7861: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.706987371641434; loss: 33.8107, focal_loss: 10.0452, kld: 23.7655\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 33.5811, focal_loss: 9.6321, kld: 23.9490: 100%|██████████████████▉| 1389/1390 [07:58<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.7121656087262798; loss: 33.5346, focal_loss: 9.5943, kld: 23.9403\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 33.3619, focal_loss: 9.3015, kld: 24.0604: 100%|██████████████████▉| 1389/1390 [07:58<00:00,  2.89it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.7125755396094522; loss: 33.3296, focal_loss: 9.2829, kld: 24.0467\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 33.4699, focal_loss: 9.2292, kld: 24.2407: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.7112055791707222; loss: 33.4330, focal_loss: 9.2068, kld: 24.2262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 47.1891, focal_loss: 24.7537, kld: 22.4354: 100%|█████████████████▉| 1389/1390 [07:57<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.5773909511209532; loss: 47.0810, focal_loss: 24.6650, kld: 22.4160\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 41.7602, focal_loss: 19.1711, kld: 22.5891: 100%|█████████████████▉| 1389/1390 [07:59<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.611389325570053; loss: 41.7700, focal_loss: 19.1944, kld: 22.5756\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 40.2407, focal_loss: 17.2657, kld: 22.9749: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.6533684925430137; loss: 40.2334, focal_loss: 17.2663, kld: 22.9671\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 39.1707, focal_loss: 15.9854, kld: 23.1853: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.92it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.6852315174377853; loss: 39.1699, focal_loss: 15.9989, kld: 23.1709\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 38.8240, focal_loss: 15.0825, kld: 23.7415: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.87it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.6944820014711517; loss: 38.7873, focal_loss: 15.0474, kld: 23.7400\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 37.6133, focal_loss: 13.5710, kld: 24.0423: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.6955529087007191; loss: 37.5948, focal_loss: 13.5665, kld: 24.0283\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 37.7204, focal_loss: 13.4916, kld: 24.2288: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.7028341448610842; loss: 37.6946, focal_loss: 13.4776, kld: 24.2170\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 37.0644, focal_loss: 12.6654, kld: 24.3990: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.90it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.7021459503166989; loss: 37.0938, focal_loss: 12.7035, kld: 24.3902\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 36.7105, focal_loss: 12.1441, kld: 24.5664: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.7059200930133958; loss: 36.6841, focal_loss: 12.1369, kld: 24.5472\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 36.5447, focal_loss: 11.8032, kld: 24.7416: 100%|████████████████▉| 1389/1390 [07:58<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.6995968260845966; loss: 36.5386, focal_loss: 11.8246, kld: 24.7140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 39.9236, focal_loss: 18.3198, kld: 21.6038: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.89it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.5365359721260768; loss: 39.8322, focal_loss: 18.2399, kld: 21.5923\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 36.5948, focal_loss: 14.9191, kld: 21.6757: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.89it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.5913215213691905; loss: 36.5478, focal_loss: 14.8824, kld: 21.6654\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 35.4489, focal_loss: 13.7616, kld: 21.6873: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.90it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.6215771367932743; loss: 35.4321, focal_loss: 13.7589, kld: 21.6732\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 34.4882, focal_loss: 12.6569, kld: 21.8313: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.89it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.6485661284898122; loss: 34.4668, focal_loss: 12.6452, kld: 21.8216\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 33.9239, focal_loss: 11.8738, kld: 22.0501: 100%|█████████████████▉| 1389/1390 [07:57<00:00,  2.90it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.6678586288767079; loss: 33.9064, focal_loss: 11.8739, kld: 22.0325\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 33.4903, focal_loss: 11.3251, kld: 22.1652: 100%|█████████████████▉| 1389/1390 [07:57<00:00,  2.94it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.6742728389681126; loss: 33.4883, focal_loss: 11.3363, kld: 22.1521\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 33.3181, focal_loss: 11.0602, kld: 22.2579: 100%|█████████████████▉| 1389/1390 [07:57<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.669531332204465; loss: 33.2966, focal_loss: 11.0536, kld: 22.2431\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 33.1306, focal_loss: 10.8168, kld: 22.3138: 100%|█████████████████▉| 1389/1390 [07:58<00:00,  2.90it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.6796375788237307; loss: 33.1264, focal_loss: 10.8285, kld: 22.2978\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 32.9910, focal_loss: 10.5493, kld: 22.4417: 100%|█████████████████▉| 1389/1390 [08:00<00:00,  2.33it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.6809519613399011; loss: 32.9695, focal_loss: 10.5493, kld: 22.4202\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 32.8649, focal_loss: 10.3396, kld: 22.5252: 100%|████████████████▉| 1389/1390 [07:51<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.6762137289735867; loss: 32.8549, focal_loss: 10.3407, kld: 22.5143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 48.0850, focal_loss: 24.4241, kld: 23.6609: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.90it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.5736325829081348; loss: 48.0139, focal_loss: 24.3623, kld: 23.6517\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 43.3741, focal_loss: 19.2992, kld: 24.0749: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.90it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.592514115385837; loss: 43.3224, focal_loss: 19.2537, kld: 24.0686\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 42.4268, focal_loss: 18.1526, kld: 24.2742: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.97it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.6057878676740912; loss: 42.4100, focal_loss: 18.1536, kld: 24.2564\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 41.5400, focal_loss: 17.0296, kld: 24.5103: 100%|█████████████████▉| 1389/1390 [07:50<00:00,  2.97it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.625479860849707; loss: 41.5195, focal_loss: 17.0191, kld: 24.5004\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 40.9351, focal_loss: 16.3550, kld: 24.5801: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.6338493763397234; loss: 40.9068, focal_loss: 16.3455, kld: 24.5612\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 39.8071, focal_loss: 15.1143, kld: 24.6928: 100%|█████████████████▉| 1389/1390 [07:50<00:00,  2.94it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.6482937001661516; loss: 39.7933, focal_loss: 15.1113, kld: 24.6819\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 39.4534, focal_loss: 14.7090, kld: 24.7443: 100%|█████████████████▉| 1389/1390 [07:50<00:00,  2.97it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.6468189667976261; loss: 39.4353, focal_loss: 14.7005, kld: 24.7348\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 39.2937, focal_loss: 14.4786, kld: 24.8151: 100%|█████████████████▉| 1389/1390 [07:50<00:00,  2.98it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.6581842529447597; loss: 39.2989, focal_loss: 14.5016, kld: 24.7973\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 39.1566, focal_loss: 14.2416, kld: 24.9150: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.94it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.6594948073899884; loss: 39.1336, focal_loss: 14.2376, kld: 24.8960\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 38.8248, focal_loss: 13.7826, kld: 25.0422: 100%|████████████████▉| 1389/1390 [07:51<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.6605481527165387; loss: 38.7995, focal_loss: 13.7688, kld: 25.0307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 41.2974, focal_loss: 18.3823, kld: 22.9151: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.99it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.5900686748367515; loss: 41.1575, focal_loss: 18.2622, kld: 22.8953\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 37.2182, focal_loss: 14.3673, kld: 22.8509: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.6143707228913137; loss: 37.1871, focal_loss: 14.3519, kld: 22.8352\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 36.3943, focal_loss: 13.5657, kld: 22.8286: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.6383145660889815; loss: 36.3601, focal_loss: 13.5463, kld: 22.8138\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 35.6129, focal_loss: 12.6852, kld: 22.9277: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.6509553666941371; loss: 35.5685, focal_loss: 12.6469, kld: 22.9216\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 35.1753, focal_loss: 12.1614, kld: 23.0139: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  3.00it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.6700637683046774; loss: 35.1225, focal_loss: 12.1235, kld: 22.9990\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 34.5387, focal_loss: 11.5068, kld: 23.0319: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.673957291070449; loss: 34.5016, focal_loss: 11.4899, kld: 23.0117\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 34.3734, focal_loss: 11.3392, kld: 23.0343: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.98it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.6775163587513314; loss: 34.3196, focal_loss: 11.2963, kld: 23.0233\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 34.2507, focal_loss: 11.1858, kld: 23.0649: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.6770696440418468; loss: 34.2453, focal_loss: 11.1950, kld: 23.0503\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 33.9858, focal_loss: 10.8213, kld: 23.1645: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.68210153991693; loss: 33.9733, focal_loss: 10.8176, kld: 23.1558\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 33.8135, focal_loss: 10.5504, kld: 23.2631: 100%|████████████████▉| 1389/1390 [07:51<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.6832861651707263; loss: 33.7543, focal_loss: 10.5146, kld: 23.2397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 52.8493, focal_loss: 29.0917, kld: 23.7576: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.85it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.407881197775933; loss: 52.7812, focal_loss: 29.0281, kld: 23.7531\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 47.3742, focal_loss: 23.4174, kld: 23.9568: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.97it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.5762956735782537; loss: 47.2776, focal_loss: 23.3425, kld: 23.9350\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 45.1485, focal_loss: 20.9378, kld: 24.2107: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.97it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.6104817146921816; loss: 45.0593, focal_loss: 20.8658, kld: 24.1935\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 44.0678, focal_loss: 19.6107, kld: 24.4571: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  3.00it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.6403714717154991; loss: 44.0089, focal_loss: 19.5708, kld: 24.4381\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 43.2191, focal_loss: 18.5907, kld: 24.6284: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.94it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.6402499211595124; loss: 43.1904, focal_loss: 18.5737, kld: 24.6167\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 42.4568, focal_loss: 17.6197, kld: 24.8371: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.6538941304979242; loss: 42.3989, focal_loss: 17.5793, kld: 24.8195\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 42.0486, focal_loss: 17.1263, kld: 24.9222: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.6588317552161657; loss: 42.0037, focal_loss: 17.0949, kld: 24.9088\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 42.0881, focal_loss: 17.0584, kld: 25.0296: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.6576564763072633; loss: 42.0372, focal_loss: 17.0152, kld: 25.0220\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 41.7240, focal_loss: 16.5353, kld: 25.1887: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.6644677679827056; loss: 41.7151, focal_loss: 16.5383, kld: 25.1768\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 41.4573, focal_loss: 16.2093, kld: 25.2480: 100%|████████████████▉| 1389/1390 [07:51<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.6705521484775538; loss: 41.4075, focal_loss: 16.1660, kld: 25.2414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 42.1381, focal_loss: 18.8428, kld: 23.2952: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.99it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.5865366258587374; loss: 41.9956, focal_loss: 18.7166, kld: 23.2790\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 38.2104, focal_loss: 14.5283, kld: 23.6820: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.99it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.6251796625710916; loss: 38.1624, focal_loss: 14.5009, kld: 23.6615\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 37.6153, focal_loss: 13.7264, kld: 23.8889: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.6391103299270772; loss: 37.5561, focal_loss: 13.6777, kld: 23.8784\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 37.2508, focal_loss: 13.0403, kld: 24.2105: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.90it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.6495954994440298; loss: 37.2271, focal_loss: 13.0257, kld: 24.2014\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 36.8428, focal_loss: 12.3220, kld: 24.5208: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.94it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.6661203994187541; loss: 36.8285, focal_loss: 12.3161, kld: 24.5125\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 36.6416, focal_loss: 11.9147, kld: 24.7270: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.98it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.6729009463115194; loss: 36.6363, focal_loss: 11.9230, kld: 24.7133\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 36.8285, focal_loss: 11.8432, kld: 24.9853: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.80it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.6743781744700599; loss: 36.7824, focal_loss: 11.8077, kld: 24.9747\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 36.8981, focal_loss: 11.4886, kld: 25.4095: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.92it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.6785786842356244; loss: 36.9246, focal_loss: 11.5237, kld: 25.4009\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 36.9742, focal_loss: 11.2679, kld: 25.7062: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.6781116269903046; loss: 36.9239, focal_loss: 11.2362, kld: 25.6878\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 36.9644, focal_loss: 10.9730, kld: 25.9914: 100%|████████████████▉| 1389/1390 [07:51<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.6790407907981318; loss: 36.9251, focal_loss: 10.9464, kld: 25.9788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 50.6436, focal_loss: 27.6982, kld: 22.9454: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.5840579967753734; loss: 50.4213, focal_loss: 27.4881, kld: 22.9333\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 43.3550, focal_loss: 20.2209, kld: 23.1340: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  3.01it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.6224938487627113; loss: 43.2390, focal_loss: 20.1280, kld: 23.1110\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 41.7002, focal_loss: 18.4983, kld: 23.2019: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.92it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.6433842220212654; loss: 41.6461, focal_loss: 18.4621, kld: 23.1840\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 40.7057, focal_loss: 17.4328, kld: 23.2729: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.6462341742491288; loss: 40.6877, focal_loss: 17.4267, kld: 23.2610\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 40.3163, focal_loss: 16.8654, kld: 23.4509: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.6654404302356707; loss: 40.2118, focal_loss: 16.7817, kld: 23.4301\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 39.4467, focal_loss: 15.8815, kld: 23.5653: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.6659235078632978; loss: 39.4503, focal_loss: 15.8980, kld: 23.5522\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 39.1796, focal_loss: 15.5519, kld: 23.6277: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.97it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.6703135444654977; loss: 39.1389, focal_loss: 15.5178, kld: 23.6211\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 38.7452, focal_loss: 15.0354, kld: 23.7098: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.92it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.6705634142745585; loss: 38.7201, focal_loss: 15.0248, kld: 23.6954\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 38.3459, focal_loss: 14.5946, kld: 23.7512: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.97it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.675654716404566; loss: 38.3217, focal_loss: 14.5848, kld: 23.7368\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 38.2664, focal_loss: 14.4313, kld: 23.8351: 100%|████████████████▉| 1389/1390 [07:51<00:00,  2.94it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.6790354017218572; loss: 38.2602, focal_loss: 14.4425, kld: 23.8177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 45.8509, focal_loss: 22.6266, kld: 23.2243: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.99it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.32235001524499574; loss: 45.7550, focal_loss: 22.5452, kld: 23.2098\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 41.7141, focal_loss: 18.5777, kld: 23.1364: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.4568007706790579; loss: 41.6554, focal_loss: 18.5373, kld: 23.1181\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 39.9022, focal_loss: 16.7935, kld: 23.1087: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.5449483691833202; loss: 39.8010, focal_loss: 16.7168, kld: 23.0841\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 38.4667, focal_loss: 15.4101, kld: 23.0566: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.97it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.5866714838531301; loss: 38.4143, focal_loss: 15.3714, kld: 23.0430\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 37.6881, focal_loss: 14.6743, kld: 23.0138: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.92it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.5997409697892764; loss: 37.6446, focal_loss: 14.6501, kld: 22.9945\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 37.4086, focal_loss: 14.4144, kld: 22.9942: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.86it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.6038999630653704; loss: 37.3539, focal_loss: 14.3750, kld: 22.9789\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 37.2082, focal_loss: 14.2221, kld: 22.9861: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.98it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.6140570929373939; loss: 37.1751, focal_loss: 14.2100, kld: 22.9651\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 36.9838, focal_loss: 14.0124, kld: 22.9714: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.6185583851256045; loss: 36.9434, focal_loss: 13.9945, kld: 22.9490\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 36.7664, focal_loss: 13.8148, kld: 22.9516: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.6223494719516586; loss: 36.7711, focal_loss: 13.8316, kld: 22.9395\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 36.6581, focal_loss: 13.7079, kld: 22.9501: 100%|████████████████▉| 1389/1390 [07:52<00:00,  2.98it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.6256217569916843; loss: 36.6494, focal_loss: 13.7176, kld: 22.9318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 53.4277, focal_loss: 33.5420, kld: 19.8858: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.92it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.35763862994767986; loss: 53.2012, focal_loss: 33.3295, kld: 19.8717\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 44.2410, focal_loss: 24.2930, kld: 19.9480: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.90it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.4688517083316531; loss: 44.1823, focal_loss: 24.2458, kld: 19.9365\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 42.8328, focal_loss: 22.7828, kld: 20.0500: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.5463747191865623; loss: 42.7483, focal_loss: 22.7069, kld: 20.0414\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 41.7628, focal_loss: 21.6072, kld: 20.1557: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.5731230376051679; loss: 41.7175, focal_loss: 21.5844, kld: 20.1331\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 41.0892, focal_loss: 20.9061, kld: 20.1831: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.97it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.5866978565900355; loss: 41.1188, focal_loss: 20.9509, kld: 20.1679\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 40.4831, focal_loss: 20.2883, kld: 20.1949: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.5963408202835783; loss: 40.4773, focal_loss: 20.2942, kld: 20.1830\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 40.3105, focal_loss: 20.1098, kld: 20.2007: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.98it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.600286194521256; loss: 40.2848, focal_loss: 20.0966, kld: 20.1882\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 40.2215, focal_loss: 20.0193, kld: 20.2022: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.6074997605410746; loss: 40.1794, focal_loss: 19.9843, kld: 20.1951\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 40.0053, focal_loss: 19.7852, kld: 20.2201: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.6130094790379634; loss: 39.9636, focal_loss: 19.7505, kld: 20.2131\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 39.8870, focal_loss: 19.6545, kld: 20.2325: 100%|████████████████▉| 1389/1390 [07:51<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.6160740408966541; loss: 39.8425, focal_loss: 19.6246, kld: 20.2179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 44.3993, focal_loss: 23.1030, kld: 21.2964: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.3416604049519897; loss: 44.3168, focal_loss: 23.0348, kld: 21.2821\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 40.4200, focal_loss: 19.0713, kld: 21.3487: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.97it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.43728054407070677; loss: 40.3319, focal_loss: 19.0011, kld: 21.3308\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 38.9327, focal_loss: 17.4797, kld: 21.4530: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.48191415734693943; loss: 38.9006, focal_loss: 17.4628, kld: 21.4378\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 38.1492, focal_loss: 16.6195, kld: 21.5297: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.99it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.518135830018276; loss: 38.1220, focal_loss: 16.6062, kld: 21.5158\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 37.5787, focal_loss: 15.9383, kld: 21.6404: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.87it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.5471902529291484; loss: 37.5066, focal_loss: 15.8888, kld: 21.6178\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 37.1250, focal_loss: 15.4496, kld: 21.6754: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.98it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.5578629945822504; loss: 37.1159, focal_loss: 15.4569, kld: 21.6590\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 36.8793, focal_loss: 15.1701, kld: 21.7092: 100%|█████████████████▉| 1389/1390 [07:53<00:00,  2.90it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.5683872055276995; loss: 36.8290, focal_loss: 15.1339, kld: 21.6951\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 36.6852, focal_loss: 14.9531, kld: 21.7321: 100%|█████████████████▉| 1389/1390 [08:06<00:00,  2.89it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.5758844450157736; loss: 36.6537, focal_loss: 14.9340, kld: 21.7197\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 36.5777, focal_loss: 14.8130, kld: 21.7648: 100%|█████████████████▉| 1389/1390 [08:09<00:00,  2.82it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.5851797627307238; loss: 36.5231, focal_loss: 14.7716, kld: 21.7515\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 36.3717, focal_loss: 14.5797, kld: 21.7920: 100%|████████████████▉| 1389/1390 [08:07<00:00,  2.89it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.5918224187263104; loss: 36.3593, focal_loss: 14.5800, kld: 21.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 56.9188, focal_loss: 35.9220, kld: 20.9967: 100%|█████████████████▉| 1389/1390 [08:07<00:00,  2.87it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.2456172097637306; loss: 56.9166, focal_loss: 35.9289, kld: 20.9877\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 52.2242, focal_loss: 31.2638, kld: 20.9604: 100%|█████████████████▉| 1389/1390 [08:17<00:00,  2.78it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.34509874357207243; loss: 52.0437, focal_loss: 31.0895, kld: 20.9542\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 46.5660, focal_loss: 25.5524, kld: 21.0136: 100%|█████████████████▉| 1389/1390 [08:09<00:00,  2.89it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.4457897475027427; loss: 46.4927, focal_loss: 25.4913, kld: 21.0014\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 44.3269, focal_loss: 23.2417, kld: 21.0851: 100%|█████████████████▉| 1389/1390 [08:04<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.519465312896805; loss: 44.2604, focal_loss: 23.1947, kld: 21.0657\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 43.3667, focal_loss: 22.2663, kld: 21.1004: 100%|█████████████████▉| 1389/1390 [07:55<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.5406117933455654; loss: 43.3627, focal_loss: 22.2827, kld: 21.0800\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 42.4967, focal_loss: 21.3812, kld: 21.1155: 100%|█████████████████▉| 1389/1390 [08:05<00:00,  2.90it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.5487758322787746; loss: 42.5096, focal_loss: 21.4100, kld: 21.0996\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 42.2494, focal_loss: 21.1351, kld: 21.1143: 100%|█████████████████▉| 1389/1390 [07:55<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.5583351577282324; loss: 42.1953, focal_loss: 21.0953, kld: 21.1000\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 41.8890, focal_loss: 20.7748, kld: 21.1142: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.99it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.5635528940263894; loss: 41.8330, focal_loss: 20.7320, kld: 21.1010\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 41.6004, focal_loss: 20.4837, kld: 21.1168: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.5683590999647538; loss: 41.5764, focal_loss: 20.4750, kld: 21.1014\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 41.4920, focal_loss: 20.3559, kld: 21.1361: 100%|████████████████▉| 1389/1390 [07:51<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.5749388180908309; loss: 41.4309, focal_loss: 20.3154, kld: 21.1155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 45.1630, focal_loss: 24.2388, kld: 20.9242: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.98it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.3215499596989051; loss: 45.0667, focal_loss: 24.1558, kld: 20.9109\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 40.9596, focal_loss: 19.9399, kld: 21.0197: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.94it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.4348850306984193; loss: 40.8799, focal_loss: 19.8739, kld: 21.0061\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 38.9484, focal_loss: 17.8663, kld: 21.0822: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.5132617686221047; loss: 38.8941, focal_loss: 17.8217, kld: 21.0724\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 37.8789, focal_loss: 16.7193, kld: 21.1596: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.86it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.5403334001307153; loss: 37.8033, focal_loss: 16.6636, kld: 21.1398\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 37.2912, focal_loss: 16.0515, kld: 21.2397: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.5532603067862975; loss: 37.2201, focal_loss: 15.9990, kld: 21.2210\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 36.8901, focal_loss: 15.6049, kld: 21.2851: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.98it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.5617891864575284; loss: 36.8887, focal_loss: 15.6122, kld: 21.2764\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 36.6434, focal_loss: 15.3297, kld: 21.3136: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.5683414969572471; loss: 36.6223, focal_loss: 15.3179, kld: 21.3044\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 36.4968, focal_loss: 15.1520, kld: 21.3448: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.576392524340746; loss: 36.4578, focal_loss: 15.1258, kld: 21.3320\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 36.2457, focal_loss: 14.8690, kld: 21.3768: 100%|█████████████████▉| 1389/1390 [07:53<00:00,  3.00it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.5813586903254374; loss: 36.2359, focal_loss: 14.8769, kld: 21.3589\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 36.1830, focal_loss: 14.7692, kld: 21.4138: 100%|████████████████▉| 1389/1390 [07:51<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.5882508665572632; loss: 36.1056, focal_loss: 14.7084, kld: 21.3972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 56.5961, focal_loss: 34.8856, kld: 21.7106: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.3370004826796585; loss: 56.4997, focal_loss: 34.7994, kld: 21.7003\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 49.3554, focal_loss: 27.6123, kld: 21.7431: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.4202690536929887; loss: 49.2501, focal_loss: 27.5211, kld: 21.7291\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 45.9322, focal_loss: 24.1234, kld: 21.8088: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.95it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.5263242019595066; loss: 45.8602, focal_loss: 24.0635, kld: 21.7967\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 44.5182, focal_loss: 22.6655, kld: 21.8526: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.92it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.5560387780349547; loss: 44.4764, focal_loss: 22.6345, kld: 21.8420\n",
      "Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 43.5043, focal_loss: 21.6412, kld: 21.8631: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.5750260121458174; loss: 43.4581, focal_loss: 21.6005, kld: 21.8577\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 42.7472, focal_loss: 20.8400, kld: 21.9072: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.87it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.5815295163279977; loss: 42.7219, focal_loss: 20.8295, kld: 21.8924\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 42.4680, focal_loss: 20.5631, kld: 21.9049: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.91it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.5866831444367091; loss: 42.3840, focal_loss: 20.4995, kld: 21.8845\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 42.2153, focal_loss: 20.3206, kld: 21.8947: 100%|█████████████████▉| 1389/1390 [07:51<00:00,  2.93it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.5912604757930133; loss: 42.1861, focal_loss: 20.3071, kld: 21.8790\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 41.9521, focal_loss: 20.0544, kld: 21.8977: 100%|█████████████████▉| 1389/1390 [07:52<00:00,  2.97it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.595028220801723; loss: 41.9139, focal_loss: 20.0296, kld: 21.8843\n",
      "Learning rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 41.7741, focal_loss: 19.8725, kld: 21.9016: 100%|████████████████▉| 1389/1390 [07:51<00:00,  2.96it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.5975803149785204; loss: 41.7603, focal_loss: 19.8793, kld: 21.8810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "VRAM_THRESHOLD = 0.9  # 90% usage (prevent leak or OOM dynamically to improve training throughput)\n",
    "# Folder paths\n",
    "result_path = \"./output/focal/Run4/\"\n",
    "data_path = \"./data/mimic/output/\"\n",
    "# Hyperparameters\n",
    "embedding_size = 256\n",
    "in_feature = embedding_size\n",
    "out_feature =embedding_size\n",
    "n_layers = 2\n",
    "reg = True\n",
    "n_heads = 1\n",
    "alpha = 0.1\n",
    "batch_size = 32\n",
    "lbd = 1\n",
    "number_of_epochs = 10\n",
    "\n",
    "# Set the hyperparameters range for grid search HPO\n",
    "lr_list = [1e-4,5e-5,1e-5]\n",
    "dropout_list = [0.2,0.3,0.4]\n",
    "focal_list = [True, False]\n",
    "\n",
    "# Set focal loss = true then switch out for false\n",
    "df_run_all = pd.DataFrame()\n",
    "for i in lr_list:\n",
    "    for j in dropout_list:\n",
    "        for k in focal_list:\n",
    "            df_run = train_evaluate(result_path,data_path,in_feature,out_feature,n_layers,reg,n_heads,\n",
    "                  alpha,batch_size,number_of_epochs,lbd,VRAM_THRESHOLD,lr=i,dropout=j,focal_loss_val=k)\n",
    "            df_run_all = pd.concat([df_run_all, df_run], ignore_index=True)\n",
    "            df_run_all.to_excel(f'{result_path}/full_summary_runs_4.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
