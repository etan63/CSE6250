{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254f7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "import argparse\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from torch.utils.data import Dataset\n",
    "import gc\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import bitsandbytes as bnb\n",
    "from torch.utils.checkpoint import checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a68a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e5a1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engmeng\\AppData\\Local\\Temp\\ipykernel_9552\\268250052.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "# Modify the train function to use autocast and return the loss as a Tensor for scaling\n",
    "def train(data, model, optim, criterion, lbd, max_clip_norm=5):\n",
    "    model.train()\n",
    "    input = data[:, :-1].to(device)\n",
    "    label = data[:, -1].float().to(device)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    \n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):  # Enable AMP here\n",
    "        logits, kld = model(input)\n",
    "        logits = logits.squeeze(-1)\n",
    "        kld = kld.sum()\n",
    "        \n",
    "        bce = criterion(logits, label)\n",
    "        loss = bce + lbd * kld\n",
    "    \n",
    "    # Scale the loss before backward pass\n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    # Apply gradient clipping only when needed\n",
    "    if max_clip_norm:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_clip_norm)\n",
    "    \n",
    "    # Update the optimizer with scaled gradients\n",
    "    scaler.step(optim)\n",
    "    scaler.update()  # Adjust scaler for next iteration\n",
    "    \n",
    "    return loss.item(), kld.item(), bce.item()\n",
    "\n",
    "\n",
    "def evaluate(model, data_iter, length):\n",
    "    model.eval()\n",
    "    y_pred = np.zeros(length)\n",
    "    y_true = np.zeros(length)\n",
    "    y_prob = np.zeros(length)\n",
    "    pointer = 0\n",
    "    for data in data_iter:\n",
    "        input = data[:, :-1].to(device)\n",
    "        label = data[:, -1]\n",
    "        batch_size = len(label)\n",
    "        probability, _ = model(input)\n",
    "        probability = torch.sigmoid(probability.squeeze(-1).detach())\n",
    "        predicted = probability > 0.5\n",
    "        y_true[pointer: pointer + batch_size] = label.cpu().numpy()\n",
    "        y_pred[pointer: pointer + batch_size] = predicted.cpu().numpy()\n",
    "        y_prob[pointer: pointer + batch_size] = probability.cpu().numpy()\n",
    "        pointer += batch_size\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    return auc(recall, precision), (y_pred, y_prob, y_true)\n",
    "\n",
    "class EHRData(Dataset):\n",
    "    def __init__(self, data, cla):\n",
    "        self.data = data\n",
    "        self.cla = cla\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cla)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.cla[idx]\n",
    "\n",
    "# Reduce the for loops as much as possible\n",
    "def collate_fn(data):\n",
    "    # Convert the sparse matrices to dense arrays in a batch operation\n",
    "    features = np.array([datum[0].toarray().ravel() for datum in data], dtype=np.float32)\n",
    "    labels = np.array([datum[1] for datum in data], dtype=np.float32).reshape(-1, 1)\n",
    "    \n",
    "    # Stack features and labels along the last axis\n",
    "    data_combined = np.hstack((features, labels))\n",
    "\n",
    "    # Convert directly to a PyTorch tensor\n",
    "    return torch.from_numpy(data_combined).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3851652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original 4 -Ok\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "def clone_params(param, N):\n",
    "    return nn.ParameterList([copy.deepcopy(param) for _ in range(N)])\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class GraphLayer(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_of_nodes,\n",
    "                 num_of_heads, dropout, alpha, concat=True):\n",
    "        super(GraphLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        self.num_of_nodes = num_of_nodes\n",
    "        self.num_of_heads = num_of_heads\n",
    "\n",
    "        # Single W Linear layer for all heads\n",
    "        self.W = nn.Linear(in_features, hidden_features * num_of_heads, bias=False)\n",
    "        self.a = nn.Parameter(torch.rand((num_of_heads, 2 * hidden_features), requires_grad=True))\n",
    "\n",
    "        # Define V based on whether heads are concatenated\n",
    "        self.V = nn.Linear(num_of_heads * hidden_features if concat else hidden_features, out_features)\n",
    "\n",
    "        # Dropout and LeakyReLU\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm = LayerNorm(num_of_heads * hidden_features if concat else hidden_features)\n",
    "\n",
    "    def initialize(self):\n",
    "        nn.init.xavier_normal_(self.W.weight.data)\n",
    "        nn.init.xavier_normal_(self.a.data)\n",
    "        nn.init.xavier_normal_(self.V.weight.data)\n",
    "\n",
    "    def attention(self, N, data, edge):\n",
    "        # Project data to (N, num_heads, hidden_features)\n",
    "        data_proj = self.W(data).view(N, self.num_of_heads, self.hidden_features)\n",
    "\n",
    "        # Gather source and destination features for each edge\n",
    "        edge_src, edge_dst = edge\n",
    "        h_src = data_proj[edge_src, :, :]  # (E, num_heads, hidden_features)\n",
    "        h_dst = data_proj[edge_dst, :, :]  # (E, num_heads, hidden_features)\n",
    "\n",
    "        # Concatenate features of edge endpoints and compute attention scores\n",
    "        edge_h = torch.cat([h_src, h_dst], dim=-1)  # (E, num_heads, 2 * hidden_features)\n",
    "        edge_e = self.leakyrelu((self.a.unsqueeze(0) * edge_h).sum(dim=-1))  # (E, num_heads)\n",
    "\n",
    "        e_rowsum = torch.zeros((N, self.num_of_heads), device=data.device)  # Shape: (N, num_heads)\n",
    "        h_prime = torch.zeros((N, self.num_of_heads, self.hidden_features), device=data.device)  # Shape: (N, num_heads, hidden_features)\n",
    "\n",
    "        # Aggregate across all edges in one pass\n",
    "        e_rowsum.index_add_(0, edge_dst, edge_e)  # Shape: (N, num_heads)\n",
    "        h_prime.index_add_(0, edge_dst, edge_e.unsqueeze(-1) * h_src)  # Shape: (N, num_heads, hidden_features)\n",
    "\n",
    "        # Normalize in-place to avoid creating new tensors\n",
    "        e_rowsum.clamp_(min=1.0)  # Prevent division by zero\n",
    "        h_prime.div_(e_rowsum.unsqueeze(-1))\n",
    "\n",
    "        return h_prime\n",
    "\n",
    "    def forward(self, edge, data):\n",
    "        N = self.num_of_nodes\n",
    "        h_prime = self.attention(N, data, edge)\n",
    "\n",
    "        # Concatenate or average heads based on `concat`\n",
    "        if self.concat:\n",
    "            h_prime = h_prime.view(N, -1)  # Concatenate heads (N, num_heads * hidden_features)\n",
    "            h_prime = F.elu(self.norm(h_prime))  # Apply ELU activation in-place\n",
    "        else:\n",
    "            h_prime = self.V(F.relu(self.norm(h_prime.mean(dim=1))))  # Apply ReLU activation in-place\n",
    "\n",
    "        # Apply dropout\n",
    "        h_prime = self.dropout(h_prime)\n",
    "\n",
    "        return h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e99c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opt 1 - OK\n",
    "class VariationalGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, num_of_nodes, n_heads, n_layers,\n",
    "                 dropout, alpha, variational=True, none_graph_features=0, concat=True):\n",
    "        super(VariationalGNN, self).__init__()\n",
    "        self.variational = variational\n",
    "        self.num_of_nodes = num_of_nodes + 1 - none_graph_features\n",
    "        self.embed = nn.Embedding(self.num_of_nodes, in_features, padding_idx=0)\n",
    "        self.in_att = clones(\n",
    "            GraphLayer(in_features, in_features, in_features, self.num_of_nodes,\n",
    "                       n_heads, dropout, alpha, concat=True), n_layers)\n",
    "        self.out_features = out_features\n",
    "        self.out_att = GraphLayer(in_features, in_features, out_features, self.num_of_nodes,\n",
    "                                  n_heads, dropout, alpha, concat=False)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.parameterize = nn.Linear(out_features, out_features * 2)\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, 1))\n",
    "        self.none_graph_features = none_graph_features\n",
    "        if none_graph_features > 0:\n",
    "            self.features_ffn = nn.Sequential(\n",
    "                nn.Linear(none_graph_features, out_features // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "            self.out_layer = nn.Sequential(\n",
    "                nn.Linear(out_features + out_features // 2, out_features),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(out_features, 1))\n",
    "        for i in range(n_layers):\n",
    "            self.in_att[i].initialize()\n",
    "\n",
    "    def data_to_edges(self, data):\n",
    "        # Convert data to edges with device allocation at the end\n",
    "        data = data.bool()\n",
    "        length = data.size()[0]\n",
    "        nonzero = data.nonzero(as_tuple=False)\n",
    "        \n",
    "        if nonzero.numel() == 0:\n",
    "            return torch.LongTensor([[0], [0]]).to(device), torch.LongTensor([[length + 1], [length + 1]]).to(device)\n",
    "        \n",
    "        if self.training:\n",
    "            mask = (torch.rand(nonzero.size(0), device=data.device) > 0.05)\n",
    "            nonzero = nonzero[mask]\n",
    "            if nonzero.numel() == 0:\n",
    "                return torch.LongTensor([[0], [0]]).to(device), torch.LongTensor([[length + 1], [length + 1]]).to(device)\n",
    "        \n",
    "        nonzero = nonzero.T + 1\n",
    "        lengths = nonzero.size(1)\n",
    "        \n",
    "        input_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                 nonzero.repeat(lengths, 1).T.contiguous().view(1, lengths ** 2)), dim=0)\n",
    "        \n",
    "        # Extend nonzero and avoid redundant device transfer\n",
    "        nonzero = torch.cat((nonzero, torch.LongTensor([[length + 1]]).to(data.device)), dim=1)\n",
    "        lengths = nonzero.size(1)\n",
    "        output_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                  nonzero.repeat(lengths, 1).T.contiguous().view(1, lengths ** 2)), dim=0)\n",
    "        return input_edges, output_edges\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = (0.5 * logvar).exp()\n",
    "            eps = torch.randn_like(std, device=mu.device)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        return mu\n",
    "\n",
    "    def encoder_decoder(self, data):\n",
    "        # Process batch all at once instead of one by one for efficiency\n",
    "        input_edges, output_edges = self.data_to_edges(data)\n",
    "        h_prime = self.embed(torch.arange(self.num_of_nodes, device=data.device).long())\n",
    "        \n",
    "        for attn in self.in_att:\n",
    "            h_prime = attn(input_edges, h_prime)\n",
    "        \n",
    "        if self.variational:\n",
    "            h_prime = self.parameterize(h_prime).view(-1, 2, self.out_features)\n",
    "            mu, logvar = h_prime[:, 0, :], h_prime[:, 1, :]\n",
    "            h_prime = self.reparameterise(mu, logvar)\n",
    "            mu, logvar = mu[data], logvar[data]\n",
    "        \n",
    "        h_prime = self.out_att(output_edges, h_prime)\n",
    "        \n",
    "        kld = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)) / mu.size(0) if self.variational else 0\n",
    "        return h_prime[-1], kld\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size = data.size(0)\n",
    "        if self.none_graph_features == 0:\n",
    "            outputs = [self.encoder_decoder(data[i]) for i in range(batch_size)]\n",
    "            outputs_h, kld_sum = zip(*outputs)\n",
    "            return self.out_layer(F.relu(torch.stack(outputs_h))), torch.sum(torch.tensor(kld_sum))\n",
    "        else:\n",
    "            outputs = [(data[i, :self.none_graph_features],\n",
    "                        self.encoder_decoder(data[i, self.none_graph_features:])) for i in range(batch_size)]\n",
    "            outputs_h, kld_sum = zip(*[(self.features_ffn(torch.FloatTensor([out[0]]).to(device)), out[1][0]) for out in outputs])\n",
    "            return self.out_layer(F.relu(torch.stack(outputs_h))), torch.sum(torch.tensor(kld_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "245d0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gradient checkpointing\n",
    "class VariationalGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, num_of_nodes, n_heads, n_layers,\n",
    "                 dropout, alpha, variational=True, none_graph_features=0, concat=True):\n",
    "        super(VariationalGNN, self).__init__()\n",
    "        self.variational = variational\n",
    "        self.num_of_nodes = num_of_nodes + 1 - none_graph_features\n",
    "        self.embed = nn.Embedding(self.num_of_nodes, in_features, padding_idx=0)\n",
    "        self.in_att = clones(\n",
    "            GraphLayer(in_features, in_features, in_features, self.num_of_nodes,\n",
    "                       n_heads, dropout, alpha, concat=True), n_layers)\n",
    "        self.out_features = out_features\n",
    "        self.out_att = GraphLayer(in_features, in_features, out_features, self.num_of_nodes,\n",
    "                                  n_heads, dropout, alpha, concat=False)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.parameterize = nn.Linear(out_features, out_features * 2)\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, 1))\n",
    "        self.none_graph_features = none_graph_features\n",
    "        if none_graph_features > 0:\n",
    "            self.features_ffn = nn.Sequential(\n",
    "                nn.Linear(none_graph_features, out_features // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "            self.out_layer = nn.Sequential(\n",
    "                nn.Linear(out_features + out_features // 2, out_features),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(out_features, 1))\n",
    "        for i in range(n_layers):\n",
    "            self.in_att[i].initialize()\n",
    "\n",
    "    def data_to_edges(self, data):\n",
    "        # Convert data to edges with device allocation at the end\n",
    "        data = data.bool()\n",
    "        length = data.size()[0]\n",
    "        nonzero = data.nonzero(as_tuple=False)\n",
    "        \n",
    "        if nonzero.numel() == 0:\n",
    "            return torch.LongTensor([[0], [0]]).to(device), torch.LongTensor([[length + 1], [length + 1]]).to(device)\n",
    "        \n",
    "        if self.training:\n",
    "            mask = (torch.rand(nonzero.size(0), device=data.device) > 0.05)\n",
    "            nonzero = nonzero[mask]\n",
    "            if nonzero.numel() == 0:\n",
    "                return torch.LongTensor([[0], [0]]).to(device), torch.LongTensor([[length + 1], [length + 1]]).to(device)\n",
    "        \n",
    "        nonzero = nonzero.T + 1\n",
    "        lengths = nonzero.size(1)\n",
    "        \n",
    "        input_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                 nonzero.repeat(lengths, 1).T.contiguous().view(1, lengths ** 2)), dim=0)\n",
    "        \n",
    "        # Extend nonzero and avoid redundant device transfer\n",
    "        nonzero = torch.cat((nonzero, torch.LongTensor([[length + 1]]).to(data.device)), dim=1)\n",
    "        lengths = nonzero.size(1)\n",
    "        output_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                  nonzero.repeat(lengths, 1).T.contiguous().view(1, lengths ** 2)), dim=0)\n",
    "        return input_edges, output_edges\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = (0.5 * logvar).exp()\n",
    "            eps = torch.randn_like(std, device=mu.device)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        return mu\n",
    "\n",
    "    def encoder_decoder(self, data):\n",
    "        # Apply gradient checkpointing to memory-intensive attention layers\n",
    "        input_edges, output_edges = self.data_to_edges(data)\n",
    "        \n",
    "        # Embed the nodes\n",
    "        h_prime = self.embed(torch.arange(self.num_of_nodes, device=data.device).long())\n",
    "        \n",
    "        # Apply gradient checkpointing on each layer in in_att\n",
    "        for attn in self.in_att:\n",
    "            h_prime = checkpoint(attn, input_edges, h_prime)\n",
    "\n",
    "        # Variational encoding step\n",
    "        if self.variational:\n",
    "            h_prime = self.parameterize(h_prime).view(-1, 2, self.out_features)\n",
    "            mu, logvar = h_prime[:, 0, :], h_prime[:, 1, :]\n",
    "            h_prime = self.reparameterise(mu, logvar)\n",
    "            mu, logvar = mu[data], logvar[data]\n",
    "\n",
    "        # Apply checkpointing to the output attention layer as well\n",
    "        h_prime = checkpoint(self.out_att, output_edges, h_prime)\n",
    "        \n",
    "        kld = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)) / mu.size(0) if self.variational else 0\n",
    "        return h_prime[-1], kld\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size = data.size(0)\n",
    "        if self.none_graph_features == 0:\n",
    "            outputs = [self.encoder_decoder(data[i]) for i in range(batch_size)]\n",
    "            outputs_h, kld_sum = zip(*outputs)\n",
    "            return self.out_layer(F.relu(torch.stack(outputs_h))), torch.sum(torch.tensor(kld_sum))\n",
    "        else:\n",
    "            outputs = [(data[i, :self.none_graph_features],\n",
    "                        self.encoder_decoder(data[i, self.none_graph_features:])) for i in range(batch_size)]\n",
    "            outputs_h, kld_sum = zip(*[(self.features_ffn(torch.FloatTensor([out[0]]).to(device)), out[1][0]) for out in outputs])\n",
    "            return self.out_layer(F.relu(torch.stack(outputs_h))), torch.sum(torch.tensor(kld_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b4288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gradient checkpointing -90% vram to speed up - Still best so far for speed vs performance\n",
    "class VariationalGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, num_of_nodes, n_heads, n_layers,\n",
    "                 dropout, alpha, variational=True, none_graph_features=0, concat=True):\n",
    "        super(VariationalGNN, self).__init__()\n",
    "        self.variational = variational\n",
    "        self.num_of_nodes = num_of_nodes + 1 - none_graph_features\n",
    "        self.embed = nn.Embedding(self.num_of_nodes, in_features, padding_idx=0)\n",
    "        self.in_att = clones(\n",
    "            GraphLayer(in_features, in_features, in_features, self.num_of_nodes,\n",
    "                       n_heads, dropout, alpha, concat=True), n_layers)\n",
    "        self.out_features = out_features\n",
    "        self.out_att = GraphLayer(in_features, in_features, out_features, self.num_of_nodes,\n",
    "                                  n_heads, dropout, alpha, concat=False)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.parameterize = nn.Linear(out_features, out_features * 2)\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, 1))\n",
    "        self.none_graph_features = none_graph_features\n",
    "        if none_graph_features > 0:\n",
    "            self.features_ffn = nn.Sequential(\n",
    "                nn.Linear(none_graph_features, out_features // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "            self.out_layer = nn.Sequential(\n",
    "                nn.Linear(out_features + out_features // 2, out_features),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(out_features, 1))\n",
    "        for i in range(n_layers):\n",
    "            self.in_att[i].initialize()\n",
    "\n",
    "    def data_to_edges(self, data):\n",
    "        # Convert data to edges with device allocation at the end\n",
    "        data = data.bool()\n",
    "        length = data.size()[0]\n",
    "        nonzero = data.nonzero(as_tuple=False)\n",
    "        \n",
    "        if nonzero.numel() == 0:\n",
    "            return torch.LongTensor([[0], [0]]).to(device), torch.LongTensor([[length + 1], [length + 1]]).to(device)\n",
    "        \n",
    "        if self.training:\n",
    "            mask = (torch.rand(nonzero.size(0), device=data.device) > 0.05)\n",
    "            nonzero = nonzero[mask]\n",
    "            if nonzero.numel() == 0:\n",
    "                return torch.LongTensor([[0], [0]]).to(device), torch.LongTensor([[length + 1], [length + 1]]).to(device)\n",
    "        \n",
    "        nonzero = nonzero.T + 1\n",
    "        lengths = nonzero.size(1)\n",
    "        \n",
    "        input_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                 nonzero.repeat(lengths, 1).T.contiguous().view(1, lengths ** 2)), dim=0)\n",
    "        \n",
    "        # Extend nonzero and avoid redundant device transfer\n",
    "        nonzero = torch.cat((nonzero, torch.LongTensor([[length + 1]]).to(data.device)), dim=1)\n",
    "        lengths = nonzero.size(1)\n",
    "        output_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                  nonzero.repeat(lengths, 1).T.contiguous().view(1, lengths ** 2)), dim=0)\n",
    "        return input_edges, output_edges\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = (0.5 * logvar).exp()\n",
    "            eps = torch.randn_like(std, device=mu.device)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        return mu\n",
    "\n",
    "    def encoder_decoder(self, data):\n",
    "        # Calculate edges\n",
    "        input_edges, output_edges = self.data_to_edges(data)\n",
    "\n",
    "        # Embed the nodes\n",
    "        h_prime = self.embed(torch.arange(self.num_of_nodes, device=data.device).long())\n",
    "\n",
    "        # Apply gradient checkpointing with VRAM monitoring\n",
    "        for attn in self.in_att:\n",
    "            # Check VRAM usage before applying checkpoint\n",
    "            if torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory > 0.9:\n",
    "                torch.cuda.empty_cache()  # Conditionally clear cache when VRAM is > 90%\n",
    "            h_prime = checkpoint(attn, input_edges, h_prime)\n",
    "\n",
    "        # Variational encoding step\n",
    "        if self.variational:\n",
    "            h_prime = self.parameterize(h_prime).view(-1, 2, self.out_features)\n",
    "            mu, logvar = h_prime[:, 0, :], h_prime[:, 1, :]\n",
    "            h_prime = self.reparameterise(mu, logvar)\n",
    "            mu, logvar = mu[data], logvar[data]\n",
    "\n",
    "        # Apply checkpointing to the output attention layer with VRAM monitoring\n",
    "        if torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory > 0.9:\n",
    "            torch.cuda.empty_cache()  # Clear cache if VRAM > 90%\n",
    "        h_prime = checkpoint(self.out_att, output_edges, h_prime)\n",
    "\n",
    "        kld = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)) / mu.size(0) if self.variational else 0\n",
    "        return h_prime[-1], kld\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size = data.size(0)\n",
    "        if self.none_graph_features == 0:\n",
    "            outputs = [self.encoder_decoder(data[i]) for i in range(batch_size)]\n",
    "            outputs_h, kld_sum = zip(*outputs)\n",
    "            return self.out_layer(F.relu(torch.stack(outputs_h))), torch.sum(torch.tensor(kld_sum))\n",
    "        else:\n",
    "            outputs = [(data[i, :self.none_graph_features],\n",
    "                        self.encoder_decoder(data[i, self.none_graph_features:])) for i in range(batch_size)]\n",
    "            outputs_h, kld_sum = zip(*[(self.features_ffn(torch.FloatTensor([out[0]]).to(device)), out[1][0]) for out in outputs])\n",
    "            return self.out_layer(F.relu(torch.stack(outputs_h))), torch.sum(torch.tensor(kld_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52cd3338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This isnt better :(\n",
    "# More complex attention\n",
    "class VariationalGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, num_of_nodes, n_heads, n_layers, \n",
    "                 dropout, alpha, variational=True, none_graph_features=0, concat=True):\n",
    "        super(VariationalGNN, self).__init__()\n",
    "        self.variational = variational\n",
    "        self.num_of_nodes = num_of_nodes + 1 - none_graph_features\n",
    "        self.embed = nn.Embedding(self.num_of_nodes, in_features, padding_idx=0)\n",
    "        self.in_att = clones(\n",
    "            GraphLayer(in_features, in_features, in_features, self.num_of_nodes, \n",
    "                       n_heads, dropout, alpha, concat=True), n_layers)\n",
    "        self.out_features = out_features\n",
    "        self.out_att = GraphLayer(in_features, in_features, out_features, self.num_of_nodes, \n",
    "                                  n_heads, dropout, alpha, concat=False)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.parameterize = nn.Linear(out_features, out_features * 2 * n_heads)\n",
    "        \n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, 1))\n",
    "        self.none_graph_features = none_graph_features\n",
    "        if none_graph_features > 0:\n",
    "            self.features_ffn = nn.Sequential(\n",
    "                nn.Linear(none_graph_features, out_features // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "            self.out_layer = nn.Sequential(\n",
    "                nn.Linear(out_features + out_features // 2, out_features),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(out_features, 1))\n",
    "        for i in range(n_layers):\n",
    "            self.in_att[i].initialize()\n",
    "\n",
    "    def data_to_edges(self, data):\n",
    "        # Convert data to edges with device allocation at the end\n",
    "        data = data.bool()\n",
    "        length = data.size()[0]\n",
    "        nonzero = data.nonzero(as_tuple=False)\n",
    "        \n",
    "        if nonzero.numel() == 0:\n",
    "            return torch.LongTensor([[0], [0]]).to(device), torch.LongTensor([[length + 1], [length + 1]]).to(device)\n",
    "        \n",
    "        if self.training:\n",
    "            mask = (torch.rand(nonzero.size(0), device=data.device) > 0.05)\n",
    "            nonzero = nonzero[mask]\n",
    "            if nonzero.numel() == 0:\n",
    "                return torch.LongTensor([[0], [0]]).to(device), torch.LongTensor([[length + 1], [length + 1]]).to(device)\n",
    "        \n",
    "        nonzero = nonzero.T + 1\n",
    "        lengths = nonzero.size(1)\n",
    "        \n",
    "        input_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                 nonzero.repeat(lengths, 1).T.contiguous().view(1, lengths ** 2)), dim=0)\n",
    "        \n",
    "        # Extend nonzero and avoid redundant device transfer\n",
    "        nonzero = torch.cat((nonzero, torch.LongTensor([[length + 1]]).to(data.device)), dim=1)\n",
    "        lengths = nonzero.size(1)\n",
    "        output_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                  nonzero.repeat(lengths, 1).T.contiguous().view(1, lengths ** 2)), dim=0)\n",
    "        return input_edges, output_edges\n",
    "\n",
    "    def reparameterise(self, mu, logvar, num_heads=3):\n",
    "        if self.training:\n",
    "            samples = []\n",
    "            for _ in range(num_heads):\n",
    "                std = (0.5 * logvar).exp()\n",
    "                eps = torch.randn_like(std, device=mu.device)\n",
    "                samples.append(eps.mul(std).add_(mu))\n",
    "            return torch.stack(samples).mean(dim=0)\n",
    "        return mu\n",
    "\n",
    "    def encoder_decoder(self, data):\n",
    "        input_edges, output_edges = self.data_to_edges(data)\n",
    "        h_prime = self.embed(torch.arange(self.num_of_nodes, device=data.device).long())\n",
    "\n",
    "        for attn in self.in_att:\n",
    "            h_residual = h_prime\n",
    "            h_prime = attn(input_edges, h_prime)\n",
    "            h_prime = F.relu(h_prime + h_residual)\n",
    "            h_prime = self.dropout(h_prime)\n",
    "        \n",
    "        if self.variational:\n",
    "            h_prime = self.parameterize(h_prime).view(-1, 2, self.out_features)\n",
    "            mu, logvar = h_prime[:, 0, :], h_prime[:, 1, :]\n",
    "            h_prime = self.reparameterise(mu, logvar)\n",
    "            mu, logvar = mu[data], logvar[data]\n",
    "\n",
    "        h_prime = self.out_att(output_edges, h_prime)\n",
    "        kld = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)) / mu.size(0) if self.variational else 0\n",
    "        return h_prime[-1], kld\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size = data.size(0)\n",
    "        if self.none_graph_features == 0:\n",
    "            outputs = [self.encoder_decoder(data[i]) for i in range(batch_size)]\n",
    "            outputs_h, kld_sum = zip(*outputs)\n",
    "            return self.out_layer(F.relu(torch.stack(outputs_h))), torch.sum(torch.tensor(kld_sum))\n",
    "        else:\n",
    "            outputs = [(data[i, :self.none_graph_features],\n",
    "                        self.encoder_decoder(data[i, self.none_graph_features:])) for i in range(batch_size)]\n",
    "            outputs_h, kld_sum = zip(*[(self.features_ffn(torch.FloatTensor([out[0]]).to(device)), out[1][0]) for out in outputs])\n",
    "            return self.out_layer(F.relu(torch.stack(outputs_h))), torch.sum(torch.tensor(kld_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This isnt better :(\n",
    "class VariationalGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, num_of_nodes, n_heads, n_layers, \n",
    "                 dropout, alpha, variational=True, none_graph_features=0, concat=True):\n",
    "        super(VariationalGNN, self).__init__()\n",
    "        self.variational = variational\n",
    "        self.num_of_nodes = num_of_nodes + 1 - none_graph_features\n",
    "        self.embed = nn.Embedding(self.num_of_nodes, in_features, padding_idx=0)\n",
    "        self.in_att = clones(\n",
    "            GraphLayer(in_features, in_features, in_features, self.num_of_nodes, \n",
    "                       n_heads, dropout, alpha, concat=True), n_layers)\n",
    "        self.out_features = out_features\n",
    "        self.out_att = GraphLayer(in_features, in_features, out_features, self.num_of_nodes, \n",
    "                                  n_heads, dropout, alpha, concat=False)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.parameterize = nn.Linear(out_features, out_features * 2 * n_heads)\n",
    "        \n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, 1))\n",
    "        self.none_graph_features = none_graph_features\n",
    "        if none_graph_features > 0:\n",
    "            self.features_ffn = nn.Sequential(\n",
    "                nn.Linear(none_graph_features, out_features // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "            self.out_layer = nn.Sequential(\n",
    "                nn.Linear(out_features + out_features // 2, out_features),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(out_features, 1))\n",
    "        for i in range(n_layers):\n",
    "            self.in_att[i].initialize()\n",
    "\n",
    "    def data_to_edges(self, data):\n",
    "        # Original data_to_edges code here\n",
    "        pass\n",
    "\n",
    "    def reparameterise(self, mu, logvar, num_heads=1):\n",
    "        if self.training and num_heads > 1:\n",
    "            samples = []\n",
    "            for _ in range(num_heads):\n",
    "                std = (0.5 * logvar).exp()\n",
    "                eps = torch.randn_like(std, device=mu.device)\n",
    "                samples.append(eps.mul(std).add_(mu))\n",
    "            return torch.stack(samples).mean(dim=0)\n",
    "        std = (0.5 * logvar).exp()\n",
    "        eps = torch.randn_like(std, device=mu.device)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def encoder_decoder(self, data):\n",
    "        input_edges, output_edges = self.data_to_edges(data)\n",
    "        h_prime = self.embed(torch.arange(self.num_of_nodes, device=data.device).long())\n",
    "\n",
    "        for attn in self.in_att:\n",
    "            h_residual = h_prime\n",
    "            h_prime = checkpoint(attn, input_edges, h_prime)\n",
    "            h_prime = F.relu(h_prime + h_residual)\n",
    "            h_prime = self.dropout(h_prime)\n",
    "        \n",
    "        if self.variational:\n",
    "            h_prime = checkpoint(self.parameterize, h_prime).view(-1, 2, self.out_features)\n",
    "            mu, logvar = h_prime[:, 0, :], h_prime[:, 1, :]\n",
    "            h_prime = self.reparameterise(mu, logvar)\n",
    "            mu, logvar = mu[data], logvar[data]\n",
    "\n",
    "        h_prime = self.out_att(output_edges, h_prime)\n",
    "        kld = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)) / mu.size(0) if self.variational else 0\n",
    "        return h_prime[-1], kld\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Original forward method code here, with optional checkpointing for `encoder_decoder`\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8639f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gradient checkpointing -90% vram to speed up - residual connections -ok and good\n",
    "class VariationalGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, num_of_nodes, n_heads, n_layers,\n",
    "                 dropout, alpha, variational=True, none_graph_features=0, concat=True):\n",
    "        super(VariationalGNN, self).__init__()\n",
    "        self.variational = variational\n",
    "        self.num_of_nodes = num_of_nodes + 1 - none_graph_features\n",
    "        self.embed = nn.Embedding(self.num_of_nodes, in_features, padding_idx=0)\n",
    "        self.in_att = clones(\n",
    "            GraphLayer(in_features, in_features, in_features, self.num_of_nodes,\n",
    "                       n_heads, dropout, alpha, concat=True), n_layers)\n",
    "        self.out_features = out_features\n",
    "        self.out_att = GraphLayer(in_features, in_features, out_features, self.num_of_nodes,\n",
    "                                  n_heads, dropout, alpha, concat=False)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.parameterize = nn.Linear(out_features, out_features * 2)\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, 1))\n",
    "        self.none_graph_features = none_graph_features\n",
    "        if none_graph_features > 0:\n",
    "            self.features_ffn = nn.Sequential(\n",
    "                nn.Linear(none_graph_features, out_features // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "            self.out_layer = nn.Sequential(\n",
    "                nn.Linear(out_features + out_features // 2, out_features),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(out_features, 1))\n",
    "        for i in range(n_layers):\n",
    "            self.in_att[i].initialize()\n",
    "\n",
    "    def data_to_edges(self, data):\n",
    "        # Convert data to edges with device allocation at the end\n",
    "        data = data.bool()\n",
    "        length = data.size()[0]\n",
    "        nonzero = data.nonzero(as_tuple=False)\n",
    "        \n",
    "        if nonzero.numel() == 0:\n",
    "            return torch.LongTensor([[0], [0]]).to(device), torch.LongTensor([[length + 1], [length + 1]]).to(device)\n",
    "        \n",
    "        if self.training:\n",
    "            mask = (torch.rand(nonzero.size(0), device=data.device) > 0.05)\n",
    "            nonzero = nonzero[mask]\n",
    "            if nonzero.numel() == 0:\n",
    "                return torch.LongTensor([[0], [0]]).to(device), torch.LongTensor([[length + 1], [length + 1]]).to(device)\n",
    "        \n",
    "        nonzero = nonzero.T + 1\n",
    "        lengths = nonzero.size(1)\n",
    "        \n",
    "        input_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                 nonzero.repeat(lengths, 1).T.contiguous().view(1, lengths ** 2)), dim=0)\n",
    "        \n",
    "        # Extend nonzero and avoid redundant device transfer\n",
    "        nonzero = torch.cat((nonzero, torch.LongTensor([[length + 1]]).to(data.device)), dim=1)\n",
    "        lengths = nonzero.size(1)\n",
    "        output_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                  nonzero.repeat(lengths, 1).T.contiguous().view(1, lengths ** 2)), dim=0)\n",
    "        return input_edges, output_edges\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = (0.5 * logvar).exp()\n",
    "            eps = torch.randn_like(std, device=mu.device)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        return mu\n",
    "\n",
    "    def encoder_decoder(self, data):\n",
    "        # Calculate edges\n",
    "        input_edges, output_edges = self.data_to_edges(data)\n",
    "\n",
    "        # Embed the nodes\n",
    "        h_prime = self.embed(torch.arange(self.num_of_nodes, device=data.device).long())\n",
    "\n",
    "        # Apply gradient checkpointing with VRAM monitoring\n",
    "        for attn in self.in_att:\n",
    "            if torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory > 0.9:\n",
    "                torch.cuda.empty_cache()  # Conditionally clear cache when VRAM is > 90%\n",
    "\n",
    "            # Residual connection for each layer in in_att\n",
    "            h_prime_res = h_prime\n",
    "            h_prime = checkpoint(attn, input_edges, h_prime) + h_prime_res  # Add residual connection\n",
    "\n",
    "        # Variational encoding step\n",
    "        if self.variational:\n",
    "            h_prime = self.parameterize(h_prime).view(-1, 2, self.out_features)\n",
    "            mu, logvar = h_prime[:, 0, :], h_prime[:, 1, :]\n",
    "            h_prime = self.reparameterise(mu, logvar)\n",
    "            mu, logvar = mu[data], logvar[data]\n",
    "\n",
    "        # Residual connection for the out_att layer as well\n",
    "        h_prime_res = h_prime\n",
    "        if torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory > 0.9:\n",
    "            torch.cuda.empty_cache()  # Clear cache if VRAM > 90%\n",
    "        h_prime = checkpoint(self.out_att, output_edges, h_prime) + h_prime_res  # Add residual connection\n",
    "\n",
    "        kld = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)) / mu.size(0) if self.variational else 0\n",
    "        return h_prime[-1], kld\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size = data.size(0)\n",
    "        if self.none_graph_features == 0:\n",
    "            outputs = [self.encoder_decoder(data[i]) for i in range(batch_size)]\n",
    "            outputs_h, kld_sum = zip(*outputs)\n",
    "            return self.out_layer(F.relu(torch.stack(outputs_h))), torch.sum(torch.tensor(kld_sum))\n",
    "        else:\n",
    "            outputs = [(data[i, :self.none_graph_features],\n",
    "                        self.encoder_decoder(data[i, self.none_graph_features:])) for i in range(batch_size)]\n",
    "            outputs_h, kld_sum = zip(*[(self.features_ffn(torch.FloatTensor([out[0]]).to(device)), out[1][0]) for out in outputs])\n",
    "            return self.out_layer(F.relu(torch.stack(outputs_h))), torch.sum(torch.tensor(kld_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b05f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "# Modify Hyperparameters here\n",
    "result_path = \"./output/\"\n",
    "data_path = \"./data/mimic/output/\"\n",
    "in_feature = embedding_size\n",
    "out_feature =embedding_size\n",
    "n_layers = 2\n",
    "lr = 1e-4\n",
    "reg = True\n",
    "n_heads = 1\n",
    "dropout = 0.2\n",
    "alpha = 0.1\n",
    "batch_size = 32\n",
    "number_of_epochs = 20\n",
    "eval_freq = 1389\n",
    "lbd = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4147c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_x, train_y = pickle.load(open(data_path + '/train_csr.pkl', 'rb'))\n",
    "val_x, val_y = pickle.load(open(data_path + '/validation_csr.pkl', 'rb'))\n",
    "test_x, test_y = pickle.load(open(data_path + '/test_csr.pkl', 'rb'))\n",
    "\n",
    "# Upsample training data\n",
    "train_upsampling = np.concatenate((np.arange(len(train_y)), np.repeat(np.where(train_y == 1)[0], 1)))\n",
    "train_x = train_x[train_upsampling]\n",
    "train_y = train_y[train_upsampling]\n",
    "\n",
    "# Create result root\n",
    "s = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "result_root = f'{result_path}/lr_{lr}-input_{embedding_size}-output_{embedding_size}-dropout_{dropout}'\n",
    "os.makedirs(result_root, exist_ok=True)\n",
    "logging.basicConfig(filename=f'{result_root}/train.log', format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "logging.info(f\"Time: {s}\")\n",
    "\n",
    "# Initialize model\n",
    "num_of_nodes = train_x.shape[1] + 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VariationalGNN(embedding_size, embedding_size, num_of_nodes, n_heads, n_layers - 1,\n",
    "                       dropout=dropout, alpha=alpha, variational=reg, none_graph_features=0).to(device)\n",
    "\n",
    "#model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "model = nn.DataParallel(model)\n",
    "val_loader = DataLoader(dataset=EHRData(val_x, val_y), batch_size=batch_size,\n",
    "                        collate_fn=collate_fn, shuffle=False\n",
    "                        #,num_workers=4\n",
    "                        #,pin_memory=True\n",
    "                       )\n",
    "# 8 bit optimizer\n",
    "optimizer = bnb.optim.Adam8bit(\n",
    "    [p for p in model.parameters() if p.requires_grad], lr=lr, weight_decay=1e-8\n",
    ")\n",
    "#optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=lr, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb0b1081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 43.5546, bce: 23.2649, kld: 20.2897: 100%|████████████████████████▉| 1389/1390 [08:29<00:00,  2.74it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.5986756537258878; loss: 43.4463, bce: 23.1734, kld: 20.2729\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 38.4648, bce: 18.0367, kld: 20.4281: 100%|████████████████████████▉| 1389/1390 [08:27<00:00,  2.77it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.6343070712910882; loss: 38.3987, bce: 17.9859, kld: 20.4128\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 37.7696, bce: 16.9939, kld: 20.7757: 100%|████████████████████████▉| 1389/1390 [08:17<00:00,  2.69it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.6510007184945322; loss: 37.7184, bce: 16.9546, kld: 20.7638\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 36.8216, bce: 15.6032, kld: 21.2184: 100%|████████████████████████▉| 1389/1390 [08:54<00:00,  2.59it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.6647215965718658; loss: 36.7798, bce: 15.5750, kld: 21.2048\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 36.0194, bce: 14.5922, kld: 21.4272: 100%|████████████████████████▉| 1389/1390 [08:54<00:00,  2.55it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.6670706979595098; loss: 36.0039, bce: 14.5923, kld: 21.4116\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 35.1038, bce: 13.4405, kld: 21.6633: 100%|████████████████████████▉| 1389/1390 [08:17<00:00,  2.86it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.6659855695603792; loss: 35.1342, bce: 13.4790, kld: 21.6552\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 34.9070, bce: 12.9890, kld: 21.9180: 100%|████████████████████████▉| 1389/1390 [08:08<00:00,  2.87it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.6748988995367622; loss: 34.8331, bce: 12.9343, kld: 21.8988\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 34.1888, bce: 12.0150, kld: 22.1738: 100%|████████████████████████▉| 1389/1390 [08:14<00:00,  2.86it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.6717231425902123; loss: 34.1422, bce: 11.9812, kld: 22.1610\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 34.1456, bce: 11.7727, kld: 22.3729: 100%|████████████████████████▉| 1389/1390 [08:36<00:00,  2.66it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.668310021243541; loss: 34.1355, bce: 11.7734, kld: 22.3621\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(t):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Train the model on this batch\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     loss, kld, bce \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlbd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_clip_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([loss, bce, kld])\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Check VRAM usage and clear cache if needed\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data, model, optim, criterion, lbd, max_clip_norm)\u001b[0m\n\u001b[0;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m bce \u001b[38;5;241m+\u001b[39m lbd \u001b[38;5;241m*\u001b[39m kld\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Scale the loss before backward pass\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Apply gradient clipping only when needed\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_clip_norm:\n",
      "File \u001b[1;32m~\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "VRAM_THRESHOLD = 0.9  # 90% usage\n",
    "\n",
    "def check_and_clear_vram(threshold=VRAM_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Check if VRAM usage exceeds the threshold; if so, clear the cache.\n",
    "    \"\"\"\n",
    "    # Get total and reserved memory\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory\n",
    "    used_vram = torch.cuda.memory_reserved(0)\n",
    "    usage_ratio = used_vram / total_vram\n",
    "    \n",
    "    # Clear cache if VRAM usage exceeds threshold\n",
    "    if usage_ratio >= threshold:\n",
    "        print(f\"Clearing VRAM cache... Current usage: {usage_ratio * 100:.2f}%\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return usage_ratio\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(number_of_epochs):\n",
    "    print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    ratio = Counter(train_y)\n",
    "    train_loader = DataLoader(dataset=EHRData(train_x, train_y), batch_size=batch_size,\n",
    "                              collate_fn=collate_fn, shuffle=True)\n",
    "    pos_weight = torch.ones(1).float().to(device) * (ratio[False] / ratio[True])\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"sum\", pos_weight=pos_weight)\n",
    "    \n",
    "    t = tqdm(iter(train_loader), leave=False, total=len(train_loader))\n",
    "    model.train()\n",
    "    total_loss = np.zeros(3)\n",
    "    \n",
    "    for idx, batch_data in enumerate(t):\n",
    "        # Train the model on this batch\n",
    "        loss, kld, bce = train(batch_data, model, optimizer, criterion, lbd, max_clip_norm=5)\n",
    "        total_loss += np.array([loss, bce, kld])\n",
    "\n",
    "        # Check VRAM usage and clear cache if needed\n",
    "        vram_usage = check_and_clear_vram()  # Check VRAM only when threshold is met\n",
    "\n",
    "        # Periodic evaluation and saving\n",
    "        if idx % eval_freq == 0 and idx > 0:\n",
    "            torch.save(model.state_dict(), f\"{result_root}/parameter{epoch}_{idx}\")\n",
    "\n",
    "            # Free resources before evaluation\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                val_auprc, _ = evaluate(model, val_loader, len(val_y))\n",
    "\n",
    "            logging.info(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "            print(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "\n",
    "            del val_auprc  # Clear evaluation results\n",
    "            model.train()\n",
    "\n",
    "        # Update progress display\n",
    "        if idx % 50 == 0 and idx > 0:\n",
    "            t.set_description(f'[epoch:{epoch + 1}] loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "            t.refresh()\n",
    "\n",
    "    # Update scheduler and check VRAM at the end of each epoch\n",
    "    scheduler.step()\n",
    "    check_and_clear_vram()  # Check VRAM usage once more after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fc7f533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 33.0723, bce: 24.5509, kld: 8.5215: 100%|█████████████████████████▉| 1389/1390 [13:20<00:00,  1.75it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.5634297207023011; loss: 32.9255, bce: 24.4059, kld: 8.5195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 28.0816, bce: 19.2732, kld: 8.8084: 100%|█████████████████████████▉| 1389/1390 [13:45<00:00,  1.65it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.632972596779331; loss: 27.9923, bce: 19.1867, kld: 8.8055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(t):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Call the modified train function with autocast enabled\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     loss, kld, bce \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlbd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_clip_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([loss, bce, kld])\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Remove unneeded variables from memory immediately after processing\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data, model, optim, criterion, lbd, max_clip_norm)\u001b[0m\n\u001b[0;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m bce \u001b[38;5;241m+\u001b[39m lbd \u001b[38;5;241m*\u001b[39m kld\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Scale the loss before backward pass\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Apply gradient clipping only when needed\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_clip_norm:\n",
      "File \u001b[1;32m~\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(number_of_epochs):\n",
    "    print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    ratio = Counter(train_y)\n",
    "    train_loader = DataLoader(dataset=EHRData(train_x, train_y), batch_size=batch_size,\n",
    "                              collate_fn=collate_fn, shuffle=True)\n",
    "    pos_weight = torch.ones(1).float().to(device) * (ratio[False] / ratio[True])\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"sum\", pos_weight=pos_weight)\n",
    "    \n",
    "    t = tqdm(iter(train_loader), leave=False, total=len(train_loader))\n",
    "    model.train()\n",
    "    total_loss = np.zeros(3)\n",
    "    \n",
    "    for idx, batch_data in enumerate(t):\n",
    "        # Call the modified train function with autocast enabled\n",
    "        loss, kld, bce = train(batch_data, model, optimizer, criterion, lbd, max_clip_norm=5)\n",
    "        total_loss += np.array([loss, bce, kld])\n",
    "\n",
    "        # Remove unneeded variables from memory immediately after processing\n",
    "        del batch_data, loss, kld, bce  # Remove references to free memory\n",
    "        gc.collect()  # Manually trigger garbage collection to free memory\n",
    "        torch.cuda.empty_cache()  # Clear GPU cache to free VRAM\n",
    "\n",
    "        # Periodic evaluation and saving\n",
    "        if idx % eval_freq == 0 and idx > 0:\n",
    "            torch.save(model.state_dict(), f\"{result_root}/parameter{epoch}_{idx}\")\n",
    "\n",
    "            # Free resources before evaluation\n",
    "            with torch.no_grad():  # Disable gradient tracking\n",
    "                model.eval()  # Set the model to evaluation mode\n",
    "                val_auprc, _ = evaluate(model, val_loader, len(val_y))\n",
    "\n",
    "            # Logging\n",
    "            logging.info(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "            print(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "\n",
    "            # Clear evaluation variables\n",
    "            del val_auprc  # Clear evaluation results if not needed anymore\n",
    "            gc.collect()  # Force garbage collection\n",
    "            torch.cuda.empty_cache()  # Free up GPU memory\n",
    "\n",
    "            # Reset the model to training mode\n",
    "            model.train()  # Switch back to training mode\n",
    "\n",
    "        # Update progress display\n",
    "        if idx % 50 == 0 and idx > 0:\n",
    "            t.set_description(f'[epoch:{epoch + 1}] loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "            t.refresh()\n",
    "\n",
    "    # After each epoch, free memory and update scheduler\n",
    "    scheduler.step()\n",
    "    gc.collect()  # Collect garbage after epoch\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache after epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67af764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83489454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory after eval!\n",
    "for epoch in range(number_of_epochs):\n",
    "    print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    ratio = Counter(train_y)\n",
    "    train_loader = DataLoader(dataset=EHRData(train_x, train_y), batch_size=batch_size,\n",
    "                              collate_fn=collate_fn, shuffle=True\n",
    "                              #,num_workers=4\n",
    "                              #,pin_memory=True\n",
    "                             )\n",
    "    pos_weight = torch.ones(1).float().to(device) * (ratio[False] / ratio[True])\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"sum\", pos_weight=pos_weight)\n",
    "    \n",
    "    t = tqdm(iter(train_loader), leave=False, total=len(train_loader))\n",
    "    model.train()\n",
    "    total_loss = np.zeros(3)\n",
    "    \n",
    "    for idx, batch_data in enumerate(t):\n",
    "        loss, kld, bce = train(batch_data, model, optimizer, criterion, lbd, 5)\n",
    "        total_loss += np.array([loss, bce, kld])\n",
    "        \n",
    "        if idx % eval_freq == 0 and idx > 0:\n",
    "            torch.save(model.state_dict(), f\"{result_root}/parameter{epoch}_{idx}\")\n",
    "\n",
    "            # Free resources before evaluation\n",
    "            with torch.no_grad():  # Disable gradient tracking\n",
    "                model.eval()  # Set the model to evaluation mode\n",
    "                val_auprc, _ = evaluate(model, val_loader, len(val_y))\n",
    "\n",
    "            logging.info(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "            print(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "\n",
    "            # Clear unnecessary variables\n",
    "            del val_auprc  # Clear evaluation results if not needed anymore\n",
    "            gc.collect()  # Run garbage collection\n",
    "            torch.cuda.empty_cache()  # Free up GPU memory\n",
    "\n",
    "            # Reset the model to training mode\n",
    "            model.train()  # Switch back to training mode\n",
    "\n",
    "        if idx % 50 == 0 and idx > 0:\n",
    "            t.set_description(f'[epoch:{epoch + 1}] loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "            t.refresh()\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57cd82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(number_of_epochs):\n",
    "    print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    ratio = Counter(train_y)\n",
    "    train_loader = DataLoader(dataset=EHRData(train_x, train_y), batch_size=batch_size,\n",
    "                              collate_fn=collate_fn, shuffle=True)\n",
    "    \n",
    "    pos_weight = torch.ones(1).float().to(device) * (ratio[False] / ratio[True])\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"sum\", pos_weight=pos_weight)\n",
    "    \n",
    "    t = tqdm(iter(train_loader), leave=False, total=len(train_loader))\n",
    "    model.train()\n",
    "    total_loss = np.zeros(3)\n",
    "    \n",
    "    for idx, batch_data in enumerate(t):\n",
    "        # Call the modified train function with autocast enabled\n",
    "        loss, kld, bce = train(batch_data, model, optimizer, criterion, lbd, max_clip_norm=5)\n",
    "        total_loss += np.array([loss, bce, kld])\n",
    "        \n",
    "        # Save model and evaluate periodically\n",
    "        if idx % eval_freq == 0 and idx > 0:\n",
    "            torch.save(model.state_dict(), f\"{result_root}/parameter{epoch}_{idx}\")\n",
    "            \n",
    "            # Free resources before evaluation\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                val_auprc, _ = evaluate(model, val_loader, len(val_y))\n",
    "\n",
    "            logging.info(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "            print(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "            \n",
    "            del val_auprc\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "        # Update progress display\n",
    "        if idx % 50 == 0 and idx > 0:\n",
    "            t.set_description(f'[epoch:{epoch + 1}] loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "            t.refresh()\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GradScaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    # Set up data loader and criterion\n",
    "    ratio = Counter(train_y)\n",
    "    train_loader = DataLoader(\n",
    "        dataset=EHRData(train_x, train_y), batch_size=batch_size,\n",
    "        collate_fn=collate_fn, shuffle=True\n",
    "    )\n",
    "    pos_weight = torch.ones(1).float().to(device) * (ratio[False] / ratio[True])\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"sum\", pos_weight=pos_weight)\n",
    "\n",
    "    # Progress bar\n",
    "    t = tqdm(iter(train_loader), leave=False, total=len(train_loader))\n",
    "    model.train()\n",
    "    total_loss = np.zeros(3)\n",
    "    \n",
    "    for idx, batch_data in enumerate(t):\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "        # Enable mixed precision for forward pass\n",
    "        with torch.cuda.amp.autocast(True):\n",
    "            loss, kld, bce = train(batch_data, model, optimizer, criterion, lbd, 5)\n",
    "        \n",
    "        # Scale loss and backpropagate\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Step optimizer with scaled gradients\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()  # Update the scaler for the next iteration\n",
    "\n",
    "        total_loss += np.array([loss.item(), bce, kld])  # Ensure loss is not a tensor here\n",
    "\n",
    "        # Evaluation and saving checkpoints\n",
    "        if idx % eval_freq == 0 and idx > 0:\n",
    "            torch.save(model.state_dict(), f\"{result_root}/parameter{epoch}_{idx}\")\n",
    "\n",
    "            with torch.no_grad():  # No gradient tracking\n",
    "                model.eval()  # Switch to eval mode\n",
    "                val_auprc, _ = evaluate(model, val_loader, len(val_y))\n",
    "\n",
    "            # Log and print evaluation results\n",
    "            logging.info(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "            print(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "\n",
    "            # Free up memory\n",
    "            del val_auprc\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            model.train()  # Switch back to training mode\n",
    "\n",
    "        # Update progress bar\n",
    "        if idx % 50 == 0 and idx > 0:\n",
    "            t.set_description(f'[epoch:{epoch + 1}] loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "            t.refresh()\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
