{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254f7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "import argparse\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from torch.utils.data import Dataset\n",
    "import gc\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import bitsandbytes as bnb\n",
    "from torch.utils.checkpoint import checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a68a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e5a1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engmeng\\AppData\\Local\\Temp\\ipykernel_13568\\3054263075.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Use AMP to reduce VRAM load and to speed up training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Modify the train function to use autocast and return the loss as a Tensor for scaling\n",
    "def train(data, model, optim, criterion, lbd, max_clip_norm=5):\n",
    "    model.train()\n",
    "    input = data[:, :-1].to(device)\n",
    "    label = data[:, -1].float().to(device)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    \n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):  # Enable AMP here\n",
    "        logits, kld = model(input)\n",
    "        logits = logits.squeeze(-1)\n",
    "        kld = kld.sum()\n",
    "        \n",
    "        bce = criterion(logits, label)\n",
    "        loss = bce + lbd * kld\n",
    "    \n",
    "    # Scale the loss before backward pass\n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    # Apply gradient clipping only when needed\n",
    "    if max_clip_norm:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_clip_norm)\n",
    "    \n",
    "    # Update the optimizer with scaled gradients\n",
    "    scaler.step(optim)\n",
    "    scaler.update()  # Adjust scaler for next iteration\n",
    "    \n",
    "    return loss.item(), kld.item(), bce.item()\n",
    "\n",
    "\n",
    "def evaluate(model, data_iter, length):\n",
    "    model.eval()\n",
    "    y_pred = np.zeros(length)\n",
    "    y_true = np.zeros(length)\n",
    "    y_prob = np.zeros(length)\n",
    "    pointer = 0\n",
    "    for data in data_iter:\n",
    "        input = data[:, :-1].to(device)\n",
    "        label = data[:, -1]\n",
    "        batch_size = len(label)\n",
    "        probability, _ = model(input)\n",
    "        probability = torch.sigmoid(probability.squeeze(-1).detach())\n",
    "        predicted = probability > 0.5\n",
    "        y_true[pointer: pointer + batch_size] = label.cpu().numpy()\n",
    "        y_pred[pointer: pointer + batch_size] = predicted.cpu().numpy()\n",
    "        y_prob[pointer: pointer + batch_size] = probability.cpu().numpy()\n",
    "        pointer += batch_size\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    return auc(recall, precision), (y_pred, y_prob, y_true)\n",
    "\n",
    "class EHRData(Dataset):\n",
    "    def __init__(self, data, cla):\n",
    "        self.data = data\n",
    "        self.cla = cla\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cla)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.cla[idx]\n",
    "\n",
    "# Reduce the for loops as much as possible\n",
    "def collate_fn(data):\n",
    "    # Convert the sparse matrices to dense arrays in a batch operation\n",
    "    features = np.array([datum[0].toarray().ravel() for datum in data], dtype=np.float32)\n",
    "    labels = np.array([datum[1] for datum in data], dtype=np.float32).reshape(-1, 1)\n",
    "    \n",
    "    # Stack features and labels along the last axis\n",
    "    data_combined = np.hstack((features, labels))\n",
    "\n",
    "    # Convert directly to a PyTorch tensor\n",
    "    return torch.from_numpy(data_combined).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3851652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "def clone_params(param, N):\n",
    "    return nn.ParameterList([copy.deepcopy(param) for _ in range(N)])\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class GraphLayer(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_of_nodes,\n",
    "                 num_of_heads, dropout, alpha, concat=True):\n",
    "        super(GraphLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        self.num_of_nodes = num_of_nodes\n",
    "        self.num_of_heads = num_of_heads\n",
    "\n",
    "        # Single W Linear layer for all heads\n",
    "        self.W = nn.Linear(in_features, hidden_features * num_of_heads, bias=False)\n",
    "        self.a = nn.Parameter(torch.rand((num_of_heads, 2 * hidden_features), requires_grad=True))\n",
    "\n",
    "        # Define V based on whether heads are concatenated\n",
    "        self.V = nn.Linear(num_of_heads * hidden_features if concat else hidden_features, out_features)\n",
    "\n",
    "        # Dropout and LeakyReLU\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm = LayerNorm(num_of_heads * hidden_features if concat else hidden_features)\n",
    "\n",
    "    def initialize(self):\n",
    "        nn.init.xavier_normal_(self.W.weight.data)\n",
    "        nn.init.xavier_normal_(self.a.data)\n",
    "        nn.init.xavier_normal_(self.V.weight.data)\n",
    "\n",
    "    def attention(self, N, data, edge):\n",
    "        # Project data to (N, num_heads, hidden_features)\n",
    "        data_proj = self.W(data).view(N, self.num_of_heads, self.hidden_features)\n",
    "\n",
    "        # Gather source and destination features for each edge\n",
    "        edge_src, edge_dst = edge\n",
    "        h_src = data_proj[edge_src, :, :]  # (E, num_heads, hidden_features)\n",
    "        h_dst = data_proj[edge_dst, :, :]  # (E, num_heads, hidden_features)\n",
    "\n",
    "        # Concatenate features of edge endpoints and compute attention scores\n",
    "        # Standard attention structure using concat\n",
    "        edge_h = torch.cat([h_src, h_dst], dim=-1)  # (E, num_heads, 2 * hidden_features)\n",
    "        edge_e = self.leakyrelu((self.a.unsqueeze(0) * edge_h).sum(dim=-1))  # (E, num_heads)\n",
    "\n",
    "        e_rowsum = torch.zeros((N, self.num_of_heads), device=data.device)  # Shape: (N, num_heads)\n",
    "        h_prime = torch.zeros((N, self.num_of_heads, self.hidden_features), device=data.device)  # Shape: (N, num_heads, hidden_features)\n",
    "\n",
    "        # Aggregate across all edges in one pass to improve efficiency\n",
    "        e_rowsum.index_add_(0, edge_dst, edge_e)  # Shape: (N, num_heads)\n",
    "        h_prime.index_add_(0, edge_dst, edge_e.unsqueeze(-1) * h_src)  # Shape: (N, num_heads, hidden_features)\n",
    "\n",
    "        # Normalize in-place to avoid creating new tensors\n",
    "        e_rowsum.clamp_(min=1.0)  # Prevent division by zero\n",
    "        h_prime.div_(e_rowsum.unsqueeze(-1))\n",
    "\n",
    "        return h_prime\n",
    "\n",
    "    def forward(self, edge, data):\n",
    "        N = self.num_of_nodes\n",
    "        h_prime = self.attention(N, data, edge)\n",
    "\n",
    "        # Concatenate or average heads based on `concat`\n",
    "        if self.concat:\n",
    "            h_prime = h_prime.view(N, -1)  # Concatenate heads (N, num_heads * hidden_features)\n",
    "            h_prime = F.elu(self.norm(h_prime))  # Apply ELU activation in-place\n",
    "        else:\n",
    "            h_prime = self.V(F.relu(self.norm(h_prime.mean(dim=1))))  # Apply ReLU activation in-place\n",
    "\n",
    "        # Apply dropout\n",
    "        h_prime = self.dropout(h_prime)\n",
    "\n",
    "        return h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8639f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gradient checkpointing -90% vram to speed up - residual connections -ok and good\n",
    "class VariationalGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, num_of_nodes, n_heads, n_layers,\n",
    "                 dropout, alpha, variational=True, none_graph_features=0, concat=True):\n",
    "        super(VariationalGNN, self).__init__()\n",
    "        self.variational = variational\n",
    "        self.num_of_nodes = num_of_nodes + 1 - none_graph_features\n",
    "        self.embed = nn.Embedding(self.num_of_nodes, in_features, padding_idx=0)\n",
    "        self.in_att = clones(\n",
    "            GraphLayer(in_features, in_features, in_features, self.num_of_nodes,\n",
    "                       n_heads, dropout, alpha, concat=True), n_layers)\n",
    "        self.out_features = out_features\n",
    "        self.out_att = GraphLayer(in_features, in_features, out_features, self.num_of_nodes,\n",
    "                                  n_heads, dropout, alpha, concat=False)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.parameterize = nn.Linear(out_features, out_features * 2)\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, 1))\n",
    "        self.none_graph_features = none_graph_features\n",
    "        if none_graph_features > 0:\n",
    "            self.features_ffn = nn.Sequential(\n",
    "                nn.Linear(none_graph_features, out_features // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "            self.out_layer = nn.Sequential(\n",
    "                nn.Linear(out_features + out_features // 2, out_features),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(out_features, 1))\n",
    "        for i in range(n_layers):\n",
    "            self.in_att[i].initialize()\n",
    "\n",
    "    def data_to_edges(self, data):\n",
    "        # Convert data to edges with device allocation at the end\n",
    "        data = data.bool()\n",
    "        length = data.size()[0]\n",
    "        nonzero = data.nonzero(as_tuple=False)\n",
    "        \n",
    "        if nonzero.numel() == 0:\n",
    "            return torch.LongTensor([[0], [0]]).to(device), torch.LongTensor([[length + 1], [length + 1]]).to(device)\n",
    "        \n",
    "        if self.training:\n",
    "            mask = (torch.rand(nonzero.size(0), device=data.device) > 0.05)\n",
    "            nonzero = nonzero[mask]\n",
    "            if nonzero.numel() == 0:\n",
    "                return torch.LongTensor([[0], [0]]).to(device), torch.LongTensor([[length + 1], [length + 1]]).to(device)\n",
    "        \n",
    "        nonzero = nonzero.T + 1\n",
    "        lengths = nonzero.size(1)\n",
    "        \n",
    "        input_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                 nonzero.repeat(lengths, 1).T.contiguous().view(1, lengths ** 2)), dim=0)\n",
    "        \n",
    "        # Extend nonzero and avoid redundant device transfer\n",
    "        nonzero = torch.cat((nonzero, torch.LongTensor([[length + 1]]).to(data.device)), dim=1)\n",
    "        lengths = nonzero.size(1)\n",
    "        output_edges = torch.cat((nonzero.repeat(1, lengths),\n",
    "                                  nonzero.repeat(lengths, 1).T.contiguous().view(1, lengths ** 2)), dim=0)\n",
    "        return input_edges, output_edges\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = (0.5 * logvar).exp()\n",
    "            eps = torch.randn_like(std, device=mu.device)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        return mu\n",
    "\n",
    "    def encoder_decoder(self, data):\n",
    "        # Calculate edges\n",
    "        input_edges, output_edges = self.data_to_edges(data)\n",
    "\n",
    "        # Embed the nodes\n",
    "        h_prime = self.embed(torch.arange(self.num_of_nodes, device=data.device).long())\n",
    "\n",
    "        # Apply gradient checkpointing with VRAM monitoring\n",
    "        for attn in self.in_att:\n",
    "            if torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory > 0.9:\n",
    "                torch.cuda.empty_cache()  # Conditionally clear cache when VRAM is > 90%\n",
    "\n",
    "            # Residual connection for each layer in in_att\n",
    "            h_prime_res = h_prime\n",
    "            h_prime = checkpoint(attn, input_edges, h_prime) + h_prime_res  # Add residual connection\n",
    "\n",
    "        # Variational encoding step\n",
    "        if self.variational:\n",
    "            h_prime = self.parameterize(h_prime).view(-1, 2, self.out_features)\n",
    "            mu, logvar = h_prime[:, 0, :], h_prime[:, 1, :]\n",
    "            h_prime = self.reparameterise(mu, logvar)\n",
    "            mu, logvar = mu[data], logvar[data]\n",
    "\n",
    "        # Residual connection for the out_att layer as well\n",
    "        h_prime_res = h_prime\n",
    "        if torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory > 0.9:\n",
    "            torch.cuda.empty_cache()  # Clear cache if VRAM > 90%\n",
    "        h_prime = checkpoint(self.out_att, output_edges, h_prime) + h_prime_res  # Add residual connection\n",
    "\n",
    "        kld = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)) / mu.size(0) if self.variational else 0\n",
    "        return h_prime[-1], kld\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size = data.size(0)\n",
    "        if self.none_graph_features == 0:\n",
    "            outputs = [self.encoder_decoder(data[i]) for i in range(batch_size)]\n",
    "            outputs_h, kld_sum = zip(*outputs)\n",
    "            return self.out_layer(F.relu(torch.stack(outputs_h))), torch.sum(torch.tensor(kld_sum))\n",
    "        else:\n",
    "            outputs = [(data[i, :self.none_graph_features],\n",
    "                        self.encoder_decoder(data[i, self.none_graph_features:])) for i in range(batch_size)]\n",
    "            outputs_h, kld_sum = zip(*[(self.features_ffn(torch.FloatTensor([out[0]]).to(device)), out[1][0]) for out in outputs])\n",
    "            return self.out_layer(F.relu(torch.stack(outputs_h))), torch.sum(torch.tensor(kld_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b05f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "# Modify Hyperparameters here\n",
    "result_path = \"./output/focal/\"\n",
    "data_path = \"./data/mimic/output/\"\n",
    "in_feature = embedding_size\n",
    "out_feature =embedding_size\n",
    "n_layers = 2\n",
    "lr = 1e-4\n",
    "reg = True\n",
    "n_heads = 1\n",
    "dropout = 0.2\n",
    "alpha = 0.1\n",
    "batch_size = 32\n",
    "number_of_epochs = 20\n",
    "eval_freq = 1389\n",
    "lbd = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4147c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_x, train_y = pickle.load(open(data_path + '/train_csr.pkl', 'rb'))\n",
    "val_x, val_y = pickle.load(open(data_path + '/validation_csr.pkl', 'rb'))\n",
    "test_x, test_y = pickle.load(open(data_path + '/test_csr.pkl', 'rb'))\n",
    "\n",
    "# Upsample training data\n",
    "train_upsampling = np.concatenate((np.arange(len(train_y)), np.repeat(np.where(train_y == 1)[0], 1)))\n",
    "train_x = train_x[train_upsampling]\n",
    "train_y = train_y[train_upsampling]\n",
    "\n",
    "# Create result root\n",
    "s = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "result_root = f'{result_path}/lr_{lr}-input_{embedding_size}-output_{embedding_size}-dropout_{dropout}'\n",
    "os.makedirs(result_root, exist_ok=True)\n",
    "logging.basicConfig(filename=f'{result_root}/train.log', format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "logging.info(f\"Time: {s}\")\n",
    "\n",
    "# Initialize model\n",
    "num_of_nodes = train_x.shape[1] + 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VariationalGNN(embedding_size, embedding_size, num_of_nodes, n_heads, n_layers - 1,\n",
    "                       dropout=dropout, alpha=alpha, variational=reg, none_graph_features=0).to(device)\n",
    "\n",
    "#model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "model = nn.DataParallel(model)\n",
    "val_loader = DataLoader(dataset=EHRData(val_x, val_y), batch_size=batch_size,\n",
    "                        collate_fn=collate_fn, shuffle=False\n",
    "                        #,num_workers=4\n",
    "                        #,pin_memory=True\n",
    "                       )\n",
    "# 8 bit optimizer\n",
    "optimizer = bnb.optim.Adam8bit(\n",
    "    [p for p in model.parameters() if p.requires_grad], lr=lr, weight_decay=1e-8\n",
    ")\n",
    "#optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=lr, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ac2505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='sum'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Compute the binary cross entropy with logits\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        # Compute the Focal Loss\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        \n",
    "        if self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca1f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 34.9347, focal_loss: 12.1687, kld: 22.7660: 100%|█████████████████▉| 1389/1390 [08:21<00:00,  2.78it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.6125607054809297; loss: 34.8669, focal_loss: 12.1141, kld: 22.7528\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 32.3792, focal_loss: 9.6794, kld: 22.6998: 100%|██████████████████▉| 1389/1390 [08:12<00:00,  2.88it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.6502812857217474; loss: 32.3136, focal_loss: 9.6358, kld: 22.6778\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 31.8232, focal_loss: 9.0713, kld: 22.7519: 100%|██████████████████▉| 1389/1390 [08:07<00:00,  2.83it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.6740615356595662; loss: 31.7760, focal_loss: 9.0348, kld: 22.7412\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 31.4774, focal_loss: 8.4389, kld: 23.0385: 100%|██████████████████▉| 1389/1390 [08:22<00:00,  2.84it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.6940983414278034; loss: 31.4497, focal_loss: 8.4242, kld: 23.0255\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 31.0887, focal_loss: 7.7224, kld: 23.3663: 100%|██████████████████▉| 1389/1390 [08:14<00:00,  2.81it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.7040336613349493; loss: 31.0746, focal_loss: 7.7182, kld: 23.3564\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 30.8127, focal_loss: 7.2689, kld: 23.5437: 100%|██████████████████▉| 1389/1390 [08:08<00:00,  2.81it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.7062720884884259; loss: 30.7924, focal_loss: 7.2611, kld: 23.5313\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 30.7645, focal_loss: 6.9350, kld: 23.8295: 100%|██████████████████▉| 1389/1390 [08:22<00:00,  2.64it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.7034420431045111; loss: 30.7806, focal_loss: 6.9604, kld: 23.8202\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 30.8012, focal_loss: 6.6988, kld: 24.1024: 100%|██████████████████▉| 1389/1390 [08:39<00:00,  2.71it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.7060466727591549; loss: 30.7557, focal_loss: 6.6685, kld: 24.0871\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 30.7516, focal_loss: 6.4074, kld: 24.3442: 100%|██████████████████▉| 1389/1390 [08:36<00:00,  2.73it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.7034668478234498; loss: 30.7303, focal_loss: 6.4011, kld: 24.3292\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:10] loss: 30.8271, focal_loss: 6.1353, kld: 24.6919: 100%|█████████████████▉| 1389/1390 [08:34<00:00,  2.73it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 AUPRC:0.7071391661058916; loss: 30.8069, focal_loss: 6.1331, kld: 24.6738\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:11] loss: 30.4801, focal_loss: 5.5361, kld: 24.9441: 100%|█████████████████▉| 1389/1390 [08:21<00:00,  2.84it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 AUPRC:0.7142407297947502; loss: 30.4593, focal_loss: 5.5292, kld: 24.9301\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:12] loss: 30.5710, focal_loss: 5.4405, kld: 25.1305: 100%|█████████████████▉| 1389/1390 [08:10<00:00,  2.83it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 AUPRC:0.7018963957676311; loss: 30.5574, focal_loss: 5.4489, kld: 25.1085\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:13] loss: 30.7626, focal_loss: 5.3473, kld: 25.4152: 100%|█████████████████▉| 1389/1390 [08:07<00:00,  2.83it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 AUPRC:0.7000335897182084; loss: 30.7783, focal_loss: 5.3748, kld: 25.4035\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:14] loss: 30.8041, focal_loss: 5.0819, kld: 25.7222: 100%|█████████████████▉| 1389/1390 [08:11<00:00,  2.88it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 AUPRC:0.6952099952693006; loss: 30.7958, focal_loss: 5.0916, kld: 25.7042\n",
      "Learning rate: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:15] loss: 31.0270, focal_loss: 4.9916, kld: 26.0354: 100%|█████████████████▉| 1389/1390 [08:10<00:00,  2.83it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 AUPRC:0.7014359958857964; loss: 31.0060, focal_loss: 4.9846, kld: 26.0213\n",
      "Learning rate: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:16] loss: 30.9828, focal_loss: 4.7290, kld: 26.2538: 100%|█████████████████▉| 1389/1390 [08:08<00:00,  2.88it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 AUPRC:0.6997258672921316; loss: 30.9501, focal_loss: 4.7134, kld: 26.2367\n",
      "Learning rate: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:17] loss: 31.0014, focal_loss: 4.5820, kld: 26.4194: 100%|█████████████████▉| 1389/1390 [08:05<00:00,  2.83it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 AUPRC:0.7000951286870355; loss: 30.9863, focal_loss: 4.5806, kld: 26.4057\n",
      "Learning rate: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:18] loss: 31.1080, focal_loss: 4.4685, kld: 26.6394:  28%|█████▎             | 386/1390 [02:26<06:31,  2.56it/s]"
     ]
    }
   ],
   "source": [
    "VRAM_THRESHOLD = 0.9  # 90% usage\n",
    "\n",
    "def check_and_clear_vram(threshold=VRAM_THRESHOLD):\n",
    "    # Get total and reserved memory\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory\n",
    "    used_vram = torch.cuda.memory_reserved(0)\n",
    "    usage_ratio = used_vram / total_vram\n",
    "    \n",
    "    # Clear cache if VRAM usage exceeds threshold\n",
    "    if usage_ratio >= threshold:\n",
    "        print(f\"Clearing VRAM cache... Current usage: {usage_ratio * 100:.2f}%\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return usage_ratio\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    ratio = Counter(train_y)\n",
    "    train_loader = DataLoader(dataset=EHRData(train_x, train_y), batch_size=batch_size,\n",
    "                              collate_fn=collate_fn, shuffle=True)\n",
    "    \n",
    "    # Initialize the Focal Loss criterion with calculated pos_weight\n",
    "    pos_weight = ratio[False] / ratio[True]\n",
    "    criterion = FocalLoss(alpha=pos_weight, gamma=2.0, reduction=\"sum\").to(device)  # Modify gamma and alpha as needed\n",
    "    \n",
    "    t = tqdm(iter(train_loader), leave=False, total=len(train_loader))\n",
    "    model.train()\n",
    "    total_loss = np.zeros(3)\n",
    "    \n",
    "    for idx, batch_data in enumerate(t):\n",
    "        # Train the model on this batch\n",
    "        loss, kld, focal_loss = train(batch_data, model, optimizer, criterion, lbd, max_clip_norm=5)\n",
    "        total_loss += np.array([loss, focal_loss, kld])\n",
    "\n",
    "        # Check VRAM usage and clear cache if needed\n",
    "        vram_usage = check_and_clear_vram()  # Check VRAM only when threshold is met\n",
    "\n",
    "        # Periodic evaluation and saving\n",
    "        if idx % eval_freq == 0 and idx > 0:\n",
    "            torch.save(model.state_dict(), f\"{result_root}/parameter{epoch}_{idx}\")\n",
    "\n",
    "            # Free resources before evaluation\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                val_auprc, _ = evaluate(model, val_loader, len(val_y))\n",
    "\n",
    "            logging.info(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, focal_loss: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "            print(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, focal_loss: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "\n",
    "            del val_auprc  # Clear evaluation results\n",
    "            model.train()\n",
    "\n",
    "        # Update progress display\n",
    "        if idx % 50 == 0 and idx > 0:\n",
    "            t.set_description(f'[epoch:{epoch + 1}] loss: {total_loss[0]/idx:.4f}, focal_loss: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "            t.refresh()\n",
    "\n",
    "    # Update scheduler and check VRAM at the end of each epoch\n",
    "    scheduler.step()\n",
    "    check_and_clear_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b1081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:1] loss: 43.5546, bce: 23.2649, kld: 20.2897: 100%|████████████████████████▉| 1389/1390 [08:29<00:00,  2.74it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 AUPRC:0.5986756537258878; loss: 43.4463, bce: 23.1734, kld: 20.2729\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:2] loss: 38.4648, bce: 18.0367, kld: 20.4281: 100%|████████████████████████▉| 1389/1390 [08:27<00:00,  2.77it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 AUPRC:0.6343070712910882; loss: 38.3987, bce: 17.9859, kld: 20.4128\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:3] loss: 37.7696, bce: 16.9939, kld: 20.7757: 100%|████████████████████████▉| 1389/1390 [08:17<00:00,  2.69it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 AUPRC:0.6510007184945322; loss: 37.7184, bce: 16.9546, kld: 20.7638\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:4] loss: 36.8216, bce: 15.6032, kld: 21.2184: 100%|████████████████████████▉| 1389/1390 [08:54<00:00,  2.59it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 AUPRC:0.6647215965718658; loss: 36.7798, bce: 15.5750, kld: 21.2048\n",
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:5] loss: 36.0194, bce: 14.5922, kld: 21.4272: 100%|████████████████████████▉| 1389/1390 [08:54<00:00,  2.55it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 AUPRC:0.6670706979595098; loss: 36.0039, bce: 14.5923, kld: 21.4116\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:6] loss: 35.1038, bce: 13.4405, kld: 21.6633: 100%|████████████████████████▉| 1389/1390 [08:17<00:00,  2.86it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 AUPRC:0.6659855695603792; loss: 35.1342, bce: 13.4790, kld: 21.6552\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:7] loss: 34.9070, bce: 12.9890, kld: 21.9180: 100%|████████████████████████▉| 1389/1390 [08:08<00:00,  2.87it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 AUPRC:0.6748988995367622; loss: 34.8331, bce: 12.9343, kld: 21.8988\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:8] loss: 34.1888, bce: 12.0150, kld: 22.1738: 100%|████████████████████████▉| 1389/1390 [08:14<00:00,  2.86it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 AUPRC:0.6717231425902123; loss: 34.1422, bce: 11.9812, kld: 22.1610\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "[epoch:9] loss: 34.1456, bce: 11.7727, kld: 22.3729: 100%|████████████████████████▉| 1389/1390 [08:36<00:00,  2.66it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 AUPRC:0.668310021243541; loss: 34.1355, bce: 11.7734, kld: 22.3621\n",
      "Learning rate: 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1390 [00:00<?, ?it/s]C:\\Users\\engmeng\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(t):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Train the model on this batch\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     loss, kld, bce \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlbd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_clip_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([loss, bce, kld])\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Check VRAM usage and clear cache if needed\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data, model, optim, criterion, lbd, max_clip_norm)\u001b[0m\n\u001b[0;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m bce \u001b[38;5;241m+\u001b[39m lbd \u001b[38;5;241m*\u001b[39m kld\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Scale the loss before backward pass\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Apply gradient clipping only when needed\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_clip_norm:\n",
      "File \u001b[1;32m~\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\gnn\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# VRAM_THRESHOLD = 0.9  # 90% usage\n",
    "\n",
    "# def check_and_clear_vram(threshold=VRAM_THRESHOLD):\n",
    "#     # Get total and reserved memory\n",
    "#     total_vram = torch.cuda.get_device_properties(0).total_memory\n",
    "#     used_vram = torch.cuda.memory_reserved(0)\n",
    "#     usage_ratio = used_vram / total_vram\n",
    "    \n",
    "#     # Clear cache if VRAM usage exceeds threshold\n",
    "#     if usage_ratio >= threshold:\n",
    "#         print(f\"Clearing VRAM cache... Current usage: {usage_ratio * 100:.2f}%\")\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "#     return usage_ratio\n",
    "\n",
    "# # Main training loop\n",
    "# for epoch in range(number_of_epochs):\n",
    "#     print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "#     ratio = Counter(train_y)\n",
    "#     train_loader = DataLoader(dataset=EHRData(train_x, train_y), batch_size=batch_size,\n",
    "#                               collate_fn=collate_fn, shuffle=True)\n",
    "#     pos_weight = torch.ones(1).float().to(device) * (ratio[False] / ratio[True])\n",
    "#     criterion = nn.BCEWithLogitsLoss(reduction=\"sum\", pos_weight=pos_weight)\n",
    "    \n",
    "#     t = tqdm(iter(train_loader), leave=False, total=len(train_loader))\n",
    "#     model.train()\n",
    "#     total_loss = np.zeros(3)\n",
    "    \n",
    "#     for idx, batch_data in enumerate(t):\n",
    "#         # Train the model on this batch\n",
    "#         loss, kld, bce = train(batch_data, model, optimizer, criterion, lbd, max_clip_norm=5)\n",
    "#         total_loss += np.array([loss, bce, kld])\n",
    "\n",
    "#         # Check VRAM usage and clear cache if needed\n",
    "#         vram_usage = check_and_clear_vram()  # Check VRAM only when threshold is met\n",
    "\n",
    "#         # Periodic evaluation and saving\n",
    "#         if idx % eval_freq == 0 and idx > 0:\n",
    "#             torch.save(model.state_dict(), f\"{result_root}/parameter{epoch}_{idx}\")\n",
    "\n",
    "#             # Free resources before evaluation\n",
    "#             with torch.no_grad():\n",
    "#                 model.eval()\n",
    "#                 val_auprc, _ = evaluate(model, val_loader, len(val_y))\n",
    "\n",
    "#             logging.info(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "#             print(f'epoch:{epoch + 1} AUPRC:{val_auprc}; loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "\n",
    "#             del val_auprc  # Clear evaluation results\n",
    "#             model.train()\n",
    "\n",
    "#         # Update progress display\n",
    "#         if idx % 50 == 0 and idx > 0:\n",
    "#             t.set_description(f'[epoch:{epoch + 1}] loss: {total_loss[0]/idx:.4f}, bce: {total_loss[1]/idx:.4f}, kld: {total_loss[2]/idx:.4f}')\n",
    "#             t.refresh()\n",
    "\n",
    "#     # Update scheduler and check VRAM at the end of each epoch\n",
    "#     scheduler.step()\n",
    "#     check_and_clear_vram()  # Check VRAM usage once more after each epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
